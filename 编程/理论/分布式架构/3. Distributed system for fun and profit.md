# 1. Distributed systems at a high level
There are two basic tasks that any computer system needs to accomplish:
- storage and
- computation
> 任何计算机系统都需要完成两项基本任务：存储和计算。

Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers - usually, because the problem no longer fits on a single computer.
> 分布式编程是用多台计算机解决在单台计算机上可以解决的同一问题的艺术--通常是因为该问题不再适合在单台计算机上解决。

However, as problem sizes increase you will reach a point where either the hardware upgrade that allows you to solve the problem on a single node does not exist, or becomes cost-prohibitive. At that point, I welcome you to the world of distributed systems.
> 但是，随着问题规模的增大，你将会遇到这样的情况：要么无法升级硬件，让你在单个节点上解决问题，要么成本过高。这时，我欢迎你来到分布式系统的世界。

It is a current reality that the best value is in mid-range, commodity hardware - as long as the maintenance costs can be kept down through fault-tolerant software.
> 目前的现实情况是，只要能通过容错软件降低维护成本，中端商品硬件的价值最高。

Ideally, adding a new machine would increase the performance and capacity of the system linearly. But of course this is not possible, because there is some overhead that arises due to having separate computers. Data needs to be copied around, computation tasks have to be coordinated and so on. This is why it's worthwhile to study distributed algorithms - they provide efficient solutions to specific problems, as well as guidance about what is possible, what the minimum cost of a correct implementation is, and what is impossible.
> 在理想情况下，增加一台新机器可以线性地提高系统的性能和容量。但这当然是不可能的，因为拥有独立的计算机会产生一些开销。数据需要复制，计算任务需要协调等等。这就是为什么值得研究分布式算法的原因--它们为特定问题提供了高效的解决方案，并指导我们了解哪些是可能的，正确实现的最低成本是多少，以及哪些是不可能的。

## What we want to achieve: Scalability and other good things
[Scalability](http://en.wikipedia.org/wiki/Scalability)：is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth.
> 可扩展性：是指一个系统、网络或流程能够以适当的方式处理不断增长的工作量，或能够扩大以适应这种增长的能力。

- Size scalability: adding more nodes should make the system linearly faster; growing the dataset should not increase latency
- Geographic scalability: it should be possible to use multiple data centers to reduce the time it takes to respond to user queries, while dealing with cross-data center latency in some sensible manner.
- Administrative scalability: adding more nodes should not increase the administrative costs of the system (e.g. the administrators-to-machines ratio).
> 
	规模可扩展性：增加节点应使系统的速度呈线性增长；数据集的增长不应增加延迟
	地理可扩展性：应该可以使用多个数据中心来减少响应用户查询所需的时间，同时以某种合理的方式处理跨数据中心的延迟。
	管理可扩展性：增加节点不应增加系统的管理成本（如管理员与机器之比）。

A scalable system is one that continues to meet the needs of its users as scale increases. There are two particularly relevant aspects - performance and availability - which can be measured in various ways.
>可扩展系统是指随着规模的扩大，仍能满足用户需求的系统。有两个特别相关的方面--性能和可用性--可以通过各种方式来衡量。

### Performance (and latency)
[Performance](http://en.wikipedia.org/wiki/Computer_performance): is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.
> 性能：是指计算机系统完成的有用工作与所用时间和资源的比较。

Depending on the context, this may involve achieving one or more of the following:
- Short response time/low latency for a given piece of work
- High throughput (rate of processing work)
- Low utilization of computing resource(s)
> 根据具体情况，这可能涉及实现以下一个或多个目标：
	特定工作的响应时间短/延迟低
	高吞吐量（处理工作的速度）
	计算资源利用率低

There are tradeoffs involved in optimizing for any of these outcomes. For example, a system may achieve a higher throughput by processing larger batches of work thereby reducing operation overhead. The tradeoff would be longer response times for individual pieces of work due to batching.
> 在对上述任何结果进行优化时，都需要权衡利弊。例如，系统可以通过处理更大批量的工作来获得更高的吞吐量，从而减少运行开销。但这样做的代价是，由于批量处理，单个工作的响应时间会更长。

Latency: The state of being latent; delay, a period between the initiation of something and the occurrence.
> 延迟：潜伏状态；延迟，某事开始和发生之间的一段时间。

Let's assume for a moment that our distributed system does just one high-level task: given a query, it takes all of the data in the system and calculates a single result. In other words, think of a distributed system as a data store with the ability to run a single deterministic computation (function) over its current content:
`result = query(all data in the system)`
> 让我们假设一下，我们的分布式系统只完成一项高级任务：给定一个查询，它获取系统中的所有数据并计算出一个结果。换句话说，可以把分布式系统看作是一个数据存储库，能够对其当前内容进行单次确定性计算（函数）：
	结果 = 查询（系统中的所有数据）

Then, what matters for latency is not the amount of old data, but rather the speed at which new data "takes effect" in the system. For example, latency could be measured in terms of how long it takes for a write to become visible to readers.
> 因此，对延迟而言，重要的不是旧数据的数量，而是新数据在系统中 "生效 "的速度。例如，延迟可以用写入到读者可见所需的时间来衡量。

In a distributed system, there is a minimum latency that cannot be overcome: the speed of light limits how fast information can travel, and hardware components have a minimum latency cost incurred per operation (think RAM and hard drives but also CPUs).
> 在分布式系统中，有一个无法逾越的最低延迟时间：光速限制了信息的传播速度，而硬件组件每次操作都会产生最低延迟成本（例如内存和硬盘，但也包括 CPU）。

How much that minimum latency impacts your queries depends on the nature of those queries and the physical distance the information needs to travel.
> 最小延迟对查询的影响程度取决于查询的性质和信息需要传输的物理距离。

### Availability (and fault tolerance)
[Availability](http://en.wikipedia.org/wiki/High_availability): the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable.
> 可用性：系统处于正常运行状态的时间比例。如果用户无法访问系统，则称系统不可用。

Distributed systems allow us to achieve desirable characteristics that would be hard to accomplish on a single system. For example, a single machine cannot tolerate any failures since it either fails or doesn't.
> 分布式系统使我们能够实现单个系统难以实现的理想特性。例如，单台机器不能容忍任何故障，因为它要么故障，要么不故障。

Systems that have no redundancy can only be as available as their underlying components. Systems built with redundancy can be tolerant of partial failures and thus be more available. It is worth noting that "redundant" can mean different things depending on what you look at - components, servers, datacenters and so on.
> 没有冗余的系统只能像其基础组件一样可用。有冗余的系统可以承受部分故障，因此可用性更高。值得注意的是，"冗余 "可以有不同的含义，这取决于你关注的是什么--组件、服务器、数据中心等。

Formulaically, availability is: `Availability = uptime / (uptime + downtime)`.
> 按公式计算，可用性为可用性 = 正常运行时间/（正常运行时间 + 停机时间）。

Availability from a technical perspective is mostly about being fault tolerant. Because the probability of a failure occurring increases with the number of components, the system should be able to compensate so as to not become less reliable as the number of components increases.
> 从技术角度看，可用性主要是指容错性。由于发生故障的概率会随着组件数量的增加而增加，因此系统应该能够进行补偿，从而不会随着组件数量的增加而降低可靠性。

Availability is in some sense a much wider concept than uptime, since the availability of a service can also be affected by, say, a network outage or the company owning the service going out of business (which would be a factor which is not really relevant to fault tolerance but would still influence the availability of the system). But without knowing every single specific aspect of the system, the best we can do is design for fault tolerance.
> 在某种意义上，可用性是一个比正常运行时间更宽泛的概念，因为服务的可用性也会受到影响，例如网络中断或拥有服务的公司倒闭（这是一个与容错无关的因素，但仍会影响系统的可用性）。但在不了解系统每个具体方面的情况下，我们能做的就是设计容错。

Fault tolerance: ability of a system to behave in a well-defined manner once faults occur.
> 容错：系统在故障发生时以明确定义的方式运行的能力。

Fault tolerance boils down to this: define what faults you expect and then design a system or an algorithm that is tolerant of them. You can't tolerate faults you haven't considered.
> 容错归根结底就是：确定你所期望的故障，然后设计出能够容错的系统或算法。你不可能容忍你没有考虑到的故障。

## What prevents us from achieving good things?
Distributed systems are constrained by two physical factors:
- the number of nodes (which increases with the required storage and computation capacity)
- the distance between nodes (information travels, at best, at the speed of light)
> 分布式系统受到两个物理因素的制约：
	节点数量（随所需存储和计算能力的增加而增加）
	节点之间的距离（信息最多以光速传播）

Working within those constraints:
- an increase in the number of independent nodes increases the probability of failure in a system (reducing availability and increasing administrative costs)
- an increase in the number of independent nodes may increase the need for communication between nodes (reducing performance as scale increases)
- an increase in geographic distance increases the minimum latency for communication between distant nodes (reducing performance for certain operations)
> 在这些限制条件下开展工作：
	增加独立节点的数量会增加系统发生故障的概率（降低可用性并增加管理成本）
	独立节点数量的增加可能会增加节点之间的通信需求（随着规模的扩大而降低性能）
	地理距离的增加会增加远距离节点间通信的最小延迟（降低某些操作的性能）

Beyond these tendencies - which are a result of the physical constraints - is the world of system design options.
> 除了这些由物理限制之外，还有系统设计方案的限制。

Both performance and availability are defined by the external guarantees the system makes. On a high level, you can think of the guarantees as the SLA (service level agreement) for the system: if I write data, how quickly can I access it elsewhere? After the data is written, what guarantees do I have of durability? If I ask the system to run a computation, how quickly will it return results? When components fail, or are taken out of operation, what impact will this have on the system?
> 性能和可用性都是由系统的外部保证来定义的。从高层次上讲，可以将这些保证视为系统的 SLA（服务水平协议）：如果我写入数据，我在其他地方访问数据的速度有多快？数据写入后，我如何保证数据的持久性？如果我要求系统运行计算，它能多快返回结果？当组件发生故障或停止运行时，会对系统产生什么影响？

There is another criterion, which is not explicitly mentioned but implied: intelligibility. How understandable are the guarantees that are made? Of course, there are no simple metrics for what is intelligible.
> 还有一个标准没有明确提及，但隐含其中：可理解性。所做的保证有多容易理解？当然，什么是可理解性并没有简单的衡量标准。

## Abstractions and models
This is where abstractions and models come into play. Abstractions make things more manageable by removing real-world aspects that are not relevant to solving a problem. Models describe the key properties of a distributed system in a precise manner. I'll discuss many kinds of models in the next chapter, such as:
- System model (asynchronous / synchronous)
- Failure model (crash-fail, partitions, Byzantine)
- Consistency model (strong, eventual)
> 这就是抽象和模型发挥作用的地方。抽象可以去除现实世界中与解决问题无关的方面，从而使问题更易于管理。模型以精确的方式描述了分布式系统的关键属性。我将在下一章讨论多种模型，如
	系统模型（异步/同步）
	故障模型（崩溃-故障、分区、拜占庭）
	一致性模型（强一致性、最终一致性）

A good abstraction makes working with a system easier to understand, while capturing the factors that are relevant for a particular purpose.
> 一个好的抽象概念可以使系统的工作更容易理解，同时还能捕捉到与特定目的相关的因素。

## Design techniques: partition and replicate
The manner in which a data set is distributed between multiple nodes is very important. In order for any computation to happen, we need to locate the data and then act on it.
> 数据集在多个节点之间分布的方式非常重要。为了进行计算，我们需要找到数据，然后对其进行操作。

There are two basic techniques that can be applied to a data set. It can be split over multiple nodes (partitioning) to allow for more parallel processing. It can also be copied or cached on different nodes to reduce the distance between the client and the server and for greater fault tolerance (replication).
> 有两种基本技术可用于数据集。可以将数据分割到多个节点上（分区），以便进行更多并行处理。也可以在不同的节点上复制或缓存数据，以缩短客户端与服务器之间的距离，提高容错能力（复制）。

The picture below illustrates the difference between these two: partitioned data (A and B below) is divided into independent sets, while replicated data (C below) is copied to multiple locations.
> 下图说明了两者之间的区别：分区数据（下图中的 A 和 B）被划分为独立的数据集，而复制数据（下图中的 C）被复制到多个位置。
![[part-repl.png]]

This is the one-two punch for solving any problem where distributed computing plays a role. Of course, the trick is in picking the right technique for your concrete implementation; there are many algorithms that implement replication and partitioning, each with different limitations and advantages which need to be assessed against your design objectives.
> 这是解决任何涉及分布式计算问题的一记重拳。当然，诀窍在于为您的具体实现选择正确的技术；有许多实现复制和分区的算法，每种算法都有不同的局限性和优势，需要根据您的设计目标进行评估。

### Partitioning
Partitioning is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data.
- Partitioning improves performance by limiting the amount of data to be examined and by locating related data in the same partition
- Partitioning improves availability by allowing partitions to fail independently, increasing the number of nodes that need to fail before availability is sacrificed
> 分区是指将数据集划分为不同的独立小集；这用于减少数据集增长的影响，因为每个分区都是数据的一个子集。
	分区可以限制需要检查的数据量，并将相关数据定位在同一分区中，从而提高性能。
	分区允许分区独立发生故障，增加了在牺牲可用性之前需要发生故障的节点数量，从而提高了可用性

Partitioning is also very much application-specific, so it is hard to say much about it without knowing the specifics. That's why the focus is on replication in most texts, including this one.
> 分区在很大程度上也与具体应用有关，因此，在不了解具体情况的情况下，很难对其进行深入探讨。这就是为什么大多数文章（包括本文章）的重点都放在复制上。

### Replication
Replication is making copies of the same data on multiple machines; this allows more servers to take part in the computation.
> 复制是指在多台机器上复制相同的数据；这样可以让更多服务器参与计算。

Replication - copying or reproducing something - is the primary way in which we can fight latency.
- Replication improves performance by making additional computing power and bandwidth applicable to a new copy of the data
- Replication improves availability by creating additional copies of the data, increasing the number of nodes that need to fail before availability is sacrificed
> 复制 - 复制或复制某些东西 - 是我们对抗延迟的主要方式。
	复制通过使额外的计算能力和带宽适用于新的数据拷贝来提高性能
	复制通过创建额外的数据副本来提高可用性，从而增加在牺牲可用性之前需要失败的节点数

Replication is about providing extra bandwidth, and caching where it counts. It is also about maintaining consistency in some way according to some consistency model.
> 复制是为了提供额外的带宽，并在重要的地方进行缓存。此外，复制还需要根据某种一致性模型，以某种方式保持一致性。

Replication allows us to achieve scalability, performance and fault tolerance. Afraid of loss of availability or reduced performance? Replicate the data to avoid a bottleneck or single point of failure. Slow computation? Replicate the computation on multiple systems. Slow I/O? Replicate the data to a local cache to reduce latency or onto multiple machines to increase throughput.
> 复制使我们能够实现可扩展性、性能和容错。害怕丧失可用性或降低性能？复制数据以避免瓶颈或单点故障。计算速度慢？在多个系统上复制计算。I/O 速度慢？将数据复制到本地缓存以减少延迟，或复制到多台机器上以提高吞吐量。

Replication is also the source of many of the problems, since there are now independent copies of the data that has to be kept in sync on multiple machines - this means ensuring that the replication follows a consistency model.
> 复制也是许多问题的根源，因为现在有独立的数据副本，必须在多台机器上保持同步--这意味着要确保复制遵循一致性模型。

The choice of a consistency model is crucial: a good consistency model provides clean semantics for programmers (in other words, the properties it guarantees are easy to reason about) and meets business/design goals such as high availability or strong consistency.
> 一致性模型的选择至关重要：一个好的一致性模型能为程序员提供简洁的语义（换句话说，它所保证的属性易于推理），并能满足业务/设计目标，如高可用性或强一致性。

Only one consistency model for replication - strong consistency - allows you to program as-if the underlying data was not replicated. Other consistency models expose some internals of the replication to the programmer. However, weaker consistency models can provide lower latency and higher availability - and are not necessarily harder to understand, just different.
> 只有一种复制的一致性模型--强一致性--允许你在编程时假设底层数据没有被复制。其他一致性模型会向程序员暴露复制的一些内部信息。然而，较弱的一致性模型可以提供更低的延迟和更高的可用性，而且并不一定更难理解，只是不同而已。

# 2. Up and down the level of abstraction
## A system model
A key property of distributed systems is distribution. More specifically, programs in a distributed system:
- run concurrently on independent nodes ...
- are connected by a network that may introduce nondeterminism and message loss ...
- and have no shared memory or shared clock.
> 分布式系统的一个关键特性是分布。更具体地说，分布式系统中的程序：
	在独立节点上并发运行...
	由网络连接，可能引入非确定性和信息丢失...
	没有共享内存或共享时钟。

There are many implications:
- each node executes a program concurrently
- knowledge is local: nodes have fast access only to their local state, and any information about global state is potentially out of date
- nodes can fail and recover from failure independently
- messages can be delayed or lost (independent of node failure; it is not easy to distinguish network failure and node failure)
- and clocks are not synchronized across nodes (local timestamps do not correspond to the global real time order, which cannot be easily observed)
> 这有很多影响：
	每个节点同时执行一个程序
	知识是本地的：节点只能快速访问其本地状态，任何有关全局状态的信息都可能是过时的
	节点可以独立发生故障并从故障中恢复
	信息可能会延迟或丢失（与节点故障无关；不易区分网络故障和节点故障）
	节点间的时钟不同步（本地时间戳与全局实时顺序不一致，不易观察到）

A system model enumerates the many assumptions associated with a particular system design.
> 系统模型列举了与特定系统设计相关的许多假设。

System model: a set of assumptions about the environment and facilities on which a distributed system is implemented
> 系统模型：关于实施分布式系统的环境和设施的一系列假设

System models vary in their assumptions about the environment and facilities. These assumptions include:
- what capabilities the nodes have and how they may fail
- how communication links operate and how they may fail and
- properties of the overall system, such as assumptions about time and order
> 系统模型对环境和设施的假设各不相同。这些假设包括
	节点具有哪些能力，它们可能如何失效
	通信链路如何运行以及可能出现的故障
	整个系统的属性，如对时间和顺序的假设

A robust system model is one that makes the weakest assumptions: any algorithm written for such a system is very tolerant of different environments, since it makes very few and very weak assumptions.
> 稳健的系统模型是一种假设最弱的模型：为这种系统编写的任何算法都能很好地容忍不同的环境，因为它的假设非常少而且非常弱。

On the other hand, we can create a system model that is easy to reason about by making strong assumptions. For example, assuming that nodes do not fail means that our algorithm does not need to handle node failures. However, such a system model is unrealistic and hence hard to apply into practice.
> 另一方面，我们可以通过做出强有力的假设来创建一个易于推理的系统模型。例如，假设节点不会发生故障，就意味着我们的算法无需处理节点故障。然而，这样的系统模型是不现实的，因此很难应用到实践中。

### Nodes in our system model
Nodes serve as hosts for computation and storage. They have:
- the ability to execute a program
- the ability to store data into volatile memory (which can be lost upon failure) and into stable state (which can be read after a failure)
- a clock (which may or may not be assumed to be accurate)
> 节点是计算和存储的主机。它们具有
	执行程序的能力
	将数据存储到易失性存储器（故障时可能丢失）和稳定状态（故障后可读取）的能力
	时钟（可能准确，也可能不准确）

Nodes execute deterministic algorithms: the local computation, the local state after the computation, and the messages sent are determined uniquely by the message received and local state when the message was received.
> 节点执行确定性算法：本地计算、计算后的本地状态和发送的信息由收到的信息和收到信息时的本地状态唯一决定。

There are many possible failure models which describe the ways in which nodes can fail. In practice, most systems assume a crash-recovery failure model: that is, nodes can only fail by crashing, and can (possibly) recover after crashing at some later point.
> 有许多可能的故障模型可以描述节点可能发生故障的方式。在实践中，大多数系统都假设了崩溃-恢复故障模型：也就是说，节点只能通过崩溃来发生故障，并（可能）在崩溃后的某个时间点恢复。

Another alternative is to assume that nodes can fail by misbehaving in any arbitrary way. This is known as [Byzantine fault tolerance](http://en.wikipedia.org/wiki/Byzantine_fault_tolerance). Byzantine faults are rarely handled in real world commercial systems, because algorithms resilient to arbitrary faults are more expensive to run and more complex to implement. I will not discuss them here.
> 另一种方法是假定节点可能以任意方式行为不当而发生故障。这就是所谓的拜占庭容错。在现实世界的商业系统中，拜占庭故障很少得到处理，因为能够抵御任意故障的算法运行成本更高，实施起来也更复杂。在此，我将不作讨论。

### Communication links in our system model
Communication links connect individual nodes to each other, and allow messages to be sent in either direction. Many books that discuss distributed algorithms assume that there are individual links between each pair of nodes, that the links provide FIFO (first in, first out) order for messages, that they can only deliver messages that were sent, and that sent messages can be lost.
> 通信链路将单个节点相互连接起来，并允许向任一方向发送信息。许多讨论分布式算法的书籍都假定，每对节点之间都有单独的链路，链路为信息提供 FIFO（先进先出）顺序，链路只能传递已发送的信息，而且已发送的信息可能会丢失。

Some algorithms assume that the network is reliable: that messages are never lost and never delayed indefinitely. This may be a reasonable assumption for some real-world settings, but in general it is preferable to consider the network to be unreliable and subject to message loss and delays.
> 有些算法假设网络是可靠的：信息不会丢失，也不会无限期延迟。对于某些实际情况来说，这种假设可能是合理的，但一般来说，最好还是将网络视为不可靠的，会出现信息丢失和延迟。

A network partition occurs when the network fails while the nodes themselves remain operational. When this occurs, messages may be lost or delayed until the network partition is repaired. Partitioned nodes may be accessible by some clients, and so must be treated differently from crashed nodes. The diagram below illustrates a node failure vs. a network partition:
> 当网络发生故障，而节点本身仍可正常运行时，就会出现网络分区。发生这种情况时，信息可能会丢失或延迟，直到网络分区被修复。被分区的节点可能会被某些客户端访问，因此必须与崩溃的节点区别对待。下图展示了节点故障与网络分区的对比：
![[system-of-2.png]]

It is rare to make further assumptions about communication links. We could assume that links only work in one direction, or we could introduce different communication costs (e.g. latency due to physical distance) for different links. However, these are rarely concerns in commercial environments except for long-distance links (WAN latency) and so I will not discuss them here; a more detailed model of costs and topology allows for better optimization at the cost of complexity.
> 对通信链路做进一步的假设是很少见的。我们可以假设链路只在一个方向上工作，也可以为不同的链路引入不同的通信成本（如物理距离造成的延迟）。不过，除了长距离链接（广域网延迟）外，商业环境中很少涉及这些问题，因此我在此不作讨论；更详细的成本和拓扑模型可以在降低复杂性的同时实现更好的优化。

### Timing / ordering assumptions
One of the consequences of physical distribution is that each node experiences the world in a unique manner. This is inescapable, because information can only travel at the speed of light. If nodes are at different distances from each other, then any messages sent from one node to the others will arrive at a different time and potentially in a different order at the other nodes.
> 物理分布的后果之一是，每个节点都以独特的方式体验世界。这是不可避免的，因为信息只能以光速传播。如果节点之间的距离不同，那么从一个节点发送到其他节点的任何信息都会在不同的时间到达其他节点，并且可能以不同的顺序到达。

Timing assumptions are a convenient shorthand for capturing assumptions about the extent to which we take this reality into account. The two main alternatives are:
- Synchronous system model: Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock
- Asynchronous system model: No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist
> 时间假设是一种方便的简写，用来描述我们在多大程度上考虑到这一现实的假设。两种主要的选择是
	同步系统模型：进程同步执行；信息传输延迟有一个已知的上限；每个进程都有一个准确的时钟
	异步系统模型：没有时序假设--例如，进程以独立的速率执行；信息传输延迟没有上限；不存在有用的时钟

The synchronous system model imposes many constraints on time and order. It essentially assumes that the nodes have the same experience: that messages that are sent are always received within a particular maximum transmission delay, and that processes execute in lock-step. This is convenient, because it allows you as the system designer to make assumptions about time and order, while the asynchronous system model doesn't.
> 同步系统模型对时间和顺序施加了许多限制。它基本上假定节点具有相同的经验：发送的信息总是在特定的最大传输延迟内收到，并且进程按部就班地执行。这很方便，因为它允许系统设计者对时间和顺序做出假设，而异步系统模型则不允许。

Asynchronicity is a non-assumption: it just assumes that you can't rely on timing (or a "time sensor").
> 异步性是一个非假设：它只是假设你不能依赖计时（或“时间传感器”）。

It is easier to solve problems in the synchronous system model, because assumptions about execution speeds, maximum message transmission delays and clock accuracy all help in solving problems since you can make inferences based on those assumptions and rule out inconvenient failure scenarios by assuming they never occur.
> 在同步系统模型中更容易解决问题，因为对执行速度、最大信息传输延迟和时钟精度的假设都有助于解决问题，因为你可以根据这些假设做出推断，并通过假设它们永远不会发生来排除不方便的故障情况。

Of course, assuming the synchronous system model is not particularly realistic. Real-world networks are subject to failures and there are no hard bounds on message delay. Real world systems are at best partially synchronous: they may occasionally work correctly and provide some upper bounds, but there will be times where messages are delayed indefinitely and clocks are out of sync. I won't really discuss algorithms for synchronous systems here, but you will probably run into them in many other introductory books because they are analytically easier (but unrealistic).
> 当然，假设同步系统模型并不特别现实。现实世界的网络可能会出现故障，而且信息延迟也没有硬约束。现实世界中的系统充其量只是部分同步：它们偶尔会正常工作并提供一些上限，但有时也会出现信息无限期延迟和时钟不同步的情况。我不会在这里讨论同步系统的算法，但你可能会在许多其他入门书籍中看到它们，因为它们在分析上更容易（但不现实）。

### The consensus problem 共识问题
Several computers (or nodes) achieve consensus if they all agree on some value. More formally:
1. Agreement: Every correct process must agree on the same value.
2. Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.
3. Termination: All processes eventually reach a decision.
4. Validity: If all correct processes propose the same value V, then all correct processes decide V.
> 如果多台计算机（或节点）都同意某个值，就能达成共识。更正式的说法是
	协议：每个正确的进程都必须就相同的值达成一致。
	完整性：每个正确的进程最多决定一个值，如果它决定了某个值，那么它一定是由某个进程提出的。
	终止：所有进程最终都会做出决定。
	有效性：如果所有正确的进程都提出了相同的值 V，那么所有正确的进程都会决定 V。

The consensus problem is at the core of many commercial distributed systems. After all, we want the reliability and performance of a distributed system without having to deal with the consequences of distribution (e.g. disagreements / divergence between nodes), and solving the consensus problem makes it possible to solve several related, more advanced problems such as atomic broadcast and atomic commit.
> 共识问题是许多商业分布式系统的核心问题。毕竟，我们想要的是分布式系统的可靠性和性能，而无需处理分布式的后果（如节点间的分歧/分歧），解决了共识问题，就有可能解决几个相关的、更高级的问题，如原子广播和原子提交。

### Two impossibility results
The first impossibility result, known as the FLP impossibility result, is an impossibility result that is particularly relevant to people who design distributed algorithms. The second - the CAP theorem - is a related result that is more relevant to practitioners; people who need to choose between different system designs but who are not directly concerned with the design of algorithms.
> 第一个不可能结果被称为 FLP 不可能结果，是一个与设计分布式算法的人特别相关的不可能结果。第二个不可能结果--CAP定理--是一个与实践者更相关的结果；实践者需要在不同的系统设计之间做出选择，但他们并不直接关注算法的设计。

## The FLP impossibility result
The FLP impossibility result (named after the authors, Fischer, Lynch and Patterson) examines the consensus problem under the asynchronous system model (technically, the agreement problem, which is a very weak form of the consensus problem). It is assumed that nodes can only fail by crashing; that the network is reliable, and that the typical timing assumptions of the asynchronous system model hold: e.g. there are no bounds on message delay.
> FLP 不可能性结果（以作者费舍尔、林奇和帕特森的名字命名）研究的是异步系统模型下的共识问题（严格来说是协议问题，是共识问题的一种非常弱的形式）。假设节点只能以崩溃的方式失败；网络是可靠的；异步系统模型的典型时序假设成立：例如，消息延迟不受约束。

Under these assumptions, the FLP result states that "there does not exist a (deterministic) algorithm for the consensus problem in an asynchronous system subject to failures, even if messages can never be lost, at most one process may fail, and it can only fail by crashing (stopping executing)".
> 在这些假设条件下，FLP 结果表明："在异步通信模型下，一个分布式系统中，即使只有一个进程可能会出现故障，也不存在一个完全正确的、确定性的算法来达成一致性"。

This result means that there is no way to solve the consensus problem under a very minimal system model in a way that cannot be delayed forever. The argument is that if such an algorithm existed, then one could devise an execution of that algorithm in which it would remain undecided ("bivalent") for an arbitrary amount of time by delaying message delivery - which is allowed in the asynchronous system model. Thus, such an algorithm cannot exist.
> 这一结果意味着，在一个极小的系统模型下，没有办法以一种不能永远延迟的方式来解决共识问题。我们的论点是，如果存在这样一种算法，那么我们就可以设计出一种算法的执行方式，通过延迟消息传递（这在异步系统模型中是允许的），在任意长的时间内保持未决（"二价"）。因此，这样的算法是不可能存在的。

This impossibility result is important because it highlights that assuming the asynchronous system model leads to a tradeoff: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold.
> 这一不可能性结果非常重要，因为它突出表明，假设采用异步系统模型会导致一种权衡：当有关消息传递界限的保证不成立时，解决共识问题的算法必须要么放弃安全性，要么放弃有效性。

This insight is particularly relevant to people who design algorithms, because it imposes a hard constraint on the problems that we know are solvable in the asynchronous system model. The CAP theorem is a related theorem that is more relevant to practitioners: it makes slightly different assumptions (network failures rather than node failures), and has more clear implications for practitioners choosing between system designs.
> 这一见解与设计算法的人员尤为相关，因为它对我们已知在异步系统模型中可以解决的问题施加了一个硬约束。CAP 定理是一个与实践者更相关的相关定理：它的假设略有不同（是网络故障而不是节点故障），对实践者选择系统设计有更明确的影响。

## The CAP theorem
The theorem states that of these three properties:
- Consistency: all nodes see the same data at the same time.
- Availability: node failures do not prevent survivors from continuing to operate.
- Partition tolerance: the system continues to operate despite message loss due to network and/or node failure
> 该定理指出，在这三个特性中：
	一致性：所有节点在同一时间看到相同的数据。
	可用性：节点故障不会妨碍幸存者继续运行。
	分区容忍性：尽管网络和/或节点故障导致信息丢失，系统仍能继续运行

only two can be satisfied simultaneously. We can even draw this as a pretty diagram, picking two properties out of three gives us three types of systems that correspond to different intersections:
> 只有两个可以同时满足。我们甚至可以把它画成一张漂亮的图，从三个属性中选出两个，就可以得到三种类型的系统，它们对应着不同的交叉点：
![[CAP.png]]

Note that the theorem states that the middle piece (having all three properties) is not achievable. Then we get three different system types:
- CA (consistency + availability). Examples include full strict quorum protocols, such as two-phase commit.
- CP (consistency + partition tolerance). Examples include majority quorum protocols in which minority partitions are unavailable such as Paxos.
- AP (availability + partition tolerance). Examples include protocols using conflict resolution, such as Dynamo.
> 请注意，该定理说明中间部分（具备所有三个属性）是无法实现的。这样，我们就得到了三种不同的系统类型：
	CA（一致性 + 可用性）。例如完全严格的法定人数协议，如两阶段提交。
	CP（一致性 + 分区容错）。例如 Paxos 等少数分区不可用的多数法定人数协议。
	AP（可用性 + 分区容错）。例子包括使用冲突解决的协议，如 Dynamo。

The CA and CP system designs both offer the same consistency model: strong consistency. The only difference is that a CA system cannot tolerate any node failures; a CP system can tolerate up to `f` faults given `2f+1` nodes in a non-Byzantine failure model (in other words, it can tolerate the failure of a minority `f` of the nodes as long as majority `f+1` stays up). The reason is simple:
- A CA system does not distinguish between node failures and network failures, and hence must stop accepting writes everywhere to avoid introducing divergence (multiple copies). It cannot tell whether a remote node is down, or whether just the network connection is down: so the only safe thing is to stop accepting writes.
- A CP system prevents divergence (e.g. maintains single-copy consistency) by forcing asymmetric behavior on the two sides of the partition. It only keeps the majority partition around, and requires the minority partition to become unavailable (e.g. stop accepting writes), which retains a degree of availability (the majority partition) and still ensures single-copy consistency.
> CA 和 CP 系统设计都提供相同的一致性模型：强一致性。唯一不同的是，CA 系统不能容忍任何节点故障；而 CP 系统在非拜占庭故障模型中，在 2f+1 个节点的情况下，最多可容忍 f 个故障（换句话说，只要多数 f+1 保持正常，它就能容忍少数 f 个节点的故障）。原因很简单：
	CA 系统无法区分节点故障和网络故障，因此必须停止接受任何地方的写入，以避免引入分歧（多副本）。它无法分辨是远程节点宕机，还是仅仅是网络连接宕机：因此唯一安全的办法就是停止接受写入。
	CP 系统通过强制分区两侧的非对称行为来防止分歧（例如，保持单副本一致性）。它只保留多数分区，而要求少数分区变得不可用（例如停止接受写入），这样就保留了一定程度的可用性（多数分区），并仍能确保单拷贝一致性。

The important thing is that CP systems incorporate network partitions into their failure model and distinguish between a majority partition and a minority partition using an algorithm like Paxos, Raft or viewstamped replication. CA systems are not partition-aware, and are historically more common: they often use the two-phase commit algorithm and are common in traditional distributed relational databases.
> 重要的是，CP 系统将网络分区纳入其故障模型，并使用 Paxos、Raft 或视图戳复制等算法区分多数分区和少数分区。CA 系统没有分区意识，在历史上比较常见：它们通常使用两阶段提交算法，在传统的分布式关系数据库中很常见。

Assuming that a partition occurs, the theorem reduces to a binary choice between availability and consistency.
> 假设发生了分区，该定理就简化为可用性和一致性之间的二元选择。
![[CAP_choice.png]]

I think there are four conclusions that should be drawn from the CAP theorem:
- First, that many system designs used in early distributed relational database systems did not take into account partition tolerance (e.g. they were CA designs). Partition tolerance is an important property for modern systems, since network partitions become much more likely if the system is geographically distributed (as many large systems are).
- Second, that there is a tension between strong consistency and high availability during network partitions. The CAP theorem is an illustration of the tradeoffs that occur between strong guarantees and distributed computation.
- Third, that _there is a tension between strong consistency and performance in normal operation_.
- Fourth - and somewhat indirectly - that _if we do not want to give up availability during a network partition, then we need to explore whether consistency models other than strong consistency are workable for our purposes_.
> 我认为应该从 CAP 定理中得出四个结论：
	首先，早期分布式关系数据库系统中使用的许多系统设计都没有考虑分区容错（例如，它们是 CA 设计）。分区容错是现代系统的一个重要特性，因为如果系统在地理上是分布式的（许多大型系统都是如此），网络分区的可能性就会大大增加。
	其次，在网络分区期间，强一致性和高可用性之间存在矛盾。CAP 定理说明了强保证和分布式计算之间的权衡。
	第三，高度一致性与正常运行性能之间存在矛盾。
	第四--有点间接--如果我们不想在网络分区期间放弃可用性，那么我们就需要探索强一致性以外的一致性模型是否能满足我们的目的。

Consistency and availability are not really binary choices, unless you limit yourself to strong consistency. But strong consistency is just one consistency model: the one where you, by necessity, need to give up availability in order to prevent more than a single copy of the data from being active. As Brewer himself points out, the "2 out of 3" interpretation is misleading.
> 一致性和可用性其实并不是二元选择，除非你把自己限制在强一致性上。但强一致性只是一种一致性模型：在这种模型中，你必须放弃可用性，以防止数据的单个副本以上处于活动状态。正如布鲁尔自己所指出的，"三选二 "的解释是有误导性的。

Consistency model: a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable
> 一致性模型：程序员和系统之间的契约，其中系统保证如果程序员遵循某些特定规则，对数据存储的操作结果将是可预测的

The "C" in CAP is "strong consistency", but "consistency" is not a synonym for "strong consistency".
> CAP 中的 "C "是 "强一致性"，但 "一致性 "并不是 "强一致性 "的同义词。

## Strong consistency vs. other consistency models
Consistency models can be categorized into two types: strong and weak consistency models:
- Strong consistency models (capable of maintaining a single copy)
    - Linearizable consistency
    - Sequential consistency
- Weak consistency models (not strong)
    - Client-centric consistency models
    - Causal consistency: strongest model available
    - Eventual consistency models
> 一致性模型可分为两类：强一致性模型和弱一致性模型：
	强一致性模型（能够保持单一副本）
		可线性化的一致性
		顺序一致性
	弱一致性模型（不强）
		以客户为中心的一致性模型
		因果一致性：现有最强模型
		最终一致性模型

Strong consistency models guarantee that the apparent order and visibility of updates is equivalent to a non-replicated system. Weak consistency models, on the other hand, do not make such guarantees.
> 强一致性模型保证更新的表观顺序和可见性等同于非复制系统。而弱一致性模型则没有这样的保证。

### Strong consistency models
Strong consistency models can further be divided into two similar, but slightly different consistency models:
- _Linearizable consistency_: Under linearizable consistency, all operations **appear** to have executed atomically in an order that is consistent with the global real-time ordering of operations. (Herlihy & Wing, 1991)
- _Sequential consistency_: Under sequential consistency, all operations **appear** to have executed atomically in some order that is consistent with the order seen at individual nodes and that is equal at all nodes. (Lamport, 1979)
> 强一致性模型又可分为两种相似但略有不同的一致性模型：
	可线性化一致性：在可线性化一致性下，所有操作似乎都以原子方式执行，其顺序与操作的全局实时排序一致。(Herlihy & Wing, 1991）
	顺序一致性：在顺序一致性条件下，所有操作似乎都是按原子顺序执行的，这种顺序与单个节点上的顺序一致，并且在所有节点上都相同。(Lamport, 1979）

The key difference is that linearizable consistency requires that the order in which operations take effect is equal to the actual real-time ordering of operations. Sequential consistency allows for operations to be reordered as long as the order observed on each node remains consistent. The only way someone can distinguish between the two is if they can observe all the inputs and timings going into the system; from the perspective of a client interacting with a node, the two are equivalent.
> 主要区别在于，可线性化一致性要求操作生效的顺序与操作的实际实时顺序相同。顺序一致性允许对操作重新排序，只要在每个节点上观察到的顺序保持一致即可。区分这两者的唯一方法就是观察进入系统的所有输入和时序；从客户端与节点交互的角度来看，两者是等价的。

### Client-centric consistency models
_Client-centric consistency models_ are consistency models that involve the notion of a client or session in some way. For example, a client-centric consistency model might guarantee that a client will never see older versions of a data item. This is often implemented by building additional caching into the client library, so that if a client moves to a replica node that contains old data, then the client library returns its cached value rather than the old value from the replica.
> 以客户为中心的一致性模型是以某种方式涉及客户或会话概念的一致性模型。例如，以客户端为中心的一致性模型可以保证客户端永远不会看到数据项的旧版本。这通常是通过在客户端库中建立额外的缓存来实现的，这样，如果客户端移动到包含旧数据的副本节点，客户端库就会返回其缓存值，而不是副本中的旧值。

Clients may still see older versions of the data, if the replica node they are on does not contain the latest version, but they will never see anomalies where an older version of a value resurfaces (e.g. because they connected to a different replica). Note that there are many kinds of consistency models that are client-centric.
> 如果客户端所在的副本节点不包含最新版本的数据，客户端可能仍会看到旧版本的数据，但绝不会看到旧版本值再次出现的异常情况（例如，因为客户端连接到了不同的副本）。请注意，以客户端为中心的一致性模型有很多种。

### Eventual consistency
The _eventual consistency_ model says that if you stop changing values, then after some undefined amount of time all replicas will agree on the same value. It is implied that before that time results between replicas are inconsistent in some undefined manner. Since it is [trivially satisfiable](http://www.bailis.org/blog/safety-and-liveness-eventual-consistency-is-not-safe/) (liveness property only), it is useless without supplemental information.
> 最终一致性模型认为，如果停止更改值，那么在某段未定义的时间后，所有副本的值将一致。这意味着在此之前，各副本之间的结果会以某种未定义的方式不一致。由于它是微不足道的可满足性（仅有效性属性），因此在没有补充信息的情况下，它是无用的。

Saying something is merely eventually consistent is like saying "people are eventually dead". It's a very weak constraint, and we'd probably want to have at least some more specific characterization of two things:
First, how long is "eventually"? It would be useful to have a strict lower bound, or at least some idea of how long it typically takes for the system to converge to the same value.
Second, how do the replicas agree on a value? A system that always returns "42" is eventually consistent: all replicas agree on the same value. It just doesn't converge to a useful value since it just keeps returning the same fixed value. Instead, we'd like to have a better idea of the method. For example, one way to decide is to have the value with the largest timestamp always win.
> 说某件事情最终是一致的，就好比说 "人最终会死"。这是一个非常薄弱的约束，我们可能希望至少对两件事有一些更具体的描述：
	首先，"最终 "是多久？如果能有一个严格的下限，或者至少知道系统收敛到相同值一般需要多长时间，那将是非常有用的。
	其次，复制如何就某个值达成一致？一个总是返回 "42 "的系统最终是一致的：所有副本都同意相同的值。但它并没有收敛到一个有用的值，因为它只是不断返回相同的固定值。相反，我们希望对方法有更好的认识。例如，一种决定方法是让时间戳最大的值总是胜出。

So when vendors say "eventual consistency", what they mean is some more precise term, such as "eventually last-writer-wins, and read-the-latest-observed-value in the meantime" consistency. The "how?" matters, because a bad method can lead to writes being lost - for example, if the clock on one node is set incorrectly and timestamps are used.
> 因此，当供应商说 "最终一致性 "时，他们指的是一些更精确的术语，比如 "最终最后写入者获胜，同时读取最新观测值 "的一致性。怎么做？"很重要，因为一个糟糕的方法可能会导致写入丢失--例如，如果一个节点上的时钟设置错误并使用了时间戳。

# 3. Time and order
Any system that can only do one thing at a time will create a total order of operations. Like people passing through a single door, every operation will have a well-defined predecessor and successor. That's basically the programming model that we've worked very hard to preserve.
> 任何一次只能做一件事的系统都会产生一个总的操作顺序。就像人们通过一扇门一样，每个操作都有明确的前置和后继操作。这基本上就是我们一直努力维护的编程模型。

The traditional model is: a single program, one process, one memory space running on one CPU. The operating system abstracts away the fact that there might be multiple CPUs and multiple programs, and that the memory on the computer is actually shared among many programs. I'm not saying that threaded programming and event-oriented programming don't exist; it's just that they are special abstractions on top of the "one/one/one" model. Programs are written to be executed in an ordered fashion: you start from the top, and then go down towards the bottom.
> 传统模式是：一个程序、一个进程、一个内存空间在一个 CPU 上运行。操作系统抽象掉了可能存在多个 CPU 和多个程序的事实，也抽象掉了计算机上的内存实际上是由许多程序共享的事实。我并不是说线程编程和面向事件编程不存在，只是它们是 "一/一/一 "模型之上的特殊抽象。程序的编写是为了以有序的方式执行：从顶层开始，然后向下执行。

Order as a property has received so much attention because the easiest way to define "correctness" is to say "it works like it would on a single machine". And that usually means that a) we run the same operations and b) that we run them in the same order - even if there are multiple machines.
> 顺序作为一种属性受到如此多的关注，是因为定义 "正确性 "的最简单方法就是说 "它能像在单台机器上一样运行"。这通常意味着：a）我们运行相同的操作；b）我们以相同的顺序运行这些操作--即使有多台机器。

The nice thing about distributed systems that preserve order (as defined for a single system) is that they are generic. You don't need to care about what the operations are, because they will be executed exactly like on a single machine. This is great because you know that you can use the same system no matter what the operations are.
> 分布式系统之所以能保持秩序（如单个系统所定义的那样），是因为它们是通用的。你不需要关心操作是什么，因为它们的执行方式与单台机器上的完全一样。这一点非常好，因为你知道，无论操作是什么，你都可以使用同一个系统。

In reality, a distributed program runs on multiple nodes; with multiple CPUs and multiple streams of operations coming in. You can still assign a total order, but it requires either accurate clocks or some form of communication. You could timestamp each operation using a completely accurate clock then use that to figure out the total order. Or you might have some kind of communication system that makes it possible to assign sequential numbers as in a total order.
> 实际上，分布式程序运行在多个节点上，有多个 CPU 和多个操作流。你仍然可以分配总顺序，但这需要精确的时钟或某种形式的通信。你可以用一个完全精确的时钟为每个操作打上时间戳，然后用它来计算总顺序。或者你也可以使用某种通信系统来分配总顺序中的顺序号。

## Total and partial order
The natural state in a distributed system is [partial order](http://en.wikipedia.org/wiki/Partially_ordered_set). Neither the network nor independent nodes make any guarantees about relative order; but at each node, you can observe a local order.
> 分布式系统的自然状态是局部秩序。网络和独立节点都不保证相对秩序；但在每个节点上，你都能观察到局部秩序。

A [total order](http://en.wikipedia.org/wiki/Total_order) is a binary relation that defines an order for every element in some set.
> 总序是一种二元关系，它定义了某个集合中每个元素的顺序。

In a system consisting of one node, a total order emerges by necessity: instructions are executed and messages are processed in a specific, observable order in a single program. We've come to rely on this total order - it makes executions of programs predictable. This order can be maintained on a distributed system, but at a cost: communication is expensive, and time synchronization is difficult and fragile.
> 在一个由单个节点组成的系统中，必然会出现一种总体秩序：在单个程序中，指令以特定的、可观察到的顺序执行，信息以特定的、可观察到的顺序处理。我们已经开始依赖这种总体秩序--它使程序的执行具有可预测性。这种秩序可以在分布式系统中保持，但代价是：通信费用昂贵，时间同步困难且脆弱。

# What is time?
Timestamps really are a shorthand value for representing the state of the world from the start of the universe to the current moment - if something occurred at a particular timestamp, then it was potentially influenced by everything that happened before it. This idea can be generalized into a causal clock that explicitly tracks causes (dependencies) rather than simply assuming that everything that preceded a timestamp was relevant. Of course, the usual assumption is that we should only worry about the state of the specific system rather than the whole world.
> 时间戳实际上是一种速记值，用来表示从宇宙开始到当前时刻的世界状态--如果某件事发生在某个特定的时间戳，那么它就有可能受到在它之前发生的所有事情的影响。这种想法可以推广到因果时钟中，明确追踪原因（依赖关系），而不是简单地假设在时间戳之前发生的所有事情都是相关的。当然，通常的假设是，我们应该只关心特定系统的状态，而不是整个世界。

Assuming that time progresses at the same rate everywhere - and that is a big assumption which I'll return to in a moment - time and timestamps have several useful interpretations when used in a program. The three interpretations are:
- Order
- Duration
- Interpretation
> 假设时间在任何地方都以相同的速度流逝--这是个很大的假设，我稍后再谈--那么在程序中使用时间和时间戳时，有几种有用的解释。这三种解释是
	顺序
	持续时间
	解释

_Order_. When I say that time is a source of order, what I mean is that:
- we can attach timestamps to unordered events to order them
- we can use timestamps to enforce a specific ordering of operations or the delivery of messages (for example, by delaying an operation if it arrives out of order)
- we can use the value of a timestamp to determine whether something happened chronologically before something else
> 秩序当我说时间是秩序的源泉时，我的意思是：
	我们可以为无序事件附加时间戳，使其有序化
	我们可以使用时间戳来强制执行操作或信息传递的特定顺序（例如，如果操作不按顺序到达，则延迟该操作）。
	我们可以使用时间戳的值来确定某件事情是否按时间顺序发生在其他事情之前

_Interpretation_ - time as a universally comparable value. The absolute value of a timestamp can be interpreted as a date, which is useful for people. Given a timestamp of when a downtime started from a log file, you can tell that it was last Saturday, when there was a [thunderstorm](https://twitter.com/AWSFail/statuses/218915147060752384).
> 解释--时间是一个普遍可比的值。时间戳的绝对值可以解释为日期，这对人们很有用。根据日志文件中停机时间的时间戳，可以知道是上周六，当时有雷雨。

_Duration_ - durations measured in time have some relation to the real world. Algorithms generally don't care about the absolute value of a clock or its interpretation as a date, but they might use durations to make some judgment calls. In particular, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency.
> 持续时间--以时间衡量的持续时间与现实世界有一定关系。算法通常并不关心时钟的绝对值或将其解释为日期，但它们可能会使用持续时间来做出一些判断。特别是，花费在等待上的时间可以提供一些线索，说明系统是被分割了，还是仅仅出现了高延迟。

By their nature, the components of distributed systems do not behave in a predictable manner. They do not guarantee any specific order, rate of advance, or lack of delay. Each node does have some local order - as execution is (roughly) sequential - but these local orders are independent of each other.
> 就其本质而言，分布式系统的各组成部分并不能以可预测的方式运行。它们不能保证任何特定的顺序、前进速度或无延迟。每个节点都有一些本地顺序--因为执行是（大致）按顺序进行的--但这些本地顺序是相互独立的。

Imposing (or assuming) order is one way to reduce the space of possible executions and possible occurrences. Humans have a hard time reasoning about things when things can happen in any order - there just are too many permutations to consider.
> 强加（或假设）秩序是减少可能执行和可能发生的空间的一种方法。当事情可能以任何顺序发生时，人类就很难对事情进行推理--要考虑的排列组合实在是太多了。

## Does time progress at the same rate everywhere?
We all have an intuitive concept of time based on our own experience as individuals. Unfortunately, that intuitive notion of time makes it easier to picture total order rather than partial order. It's easier to picture a sequence in which things happen one after another, rather than concurrently. It is easier to reason about a single order of messages than to reason about messages arriving in different orders and with different delays.
> 我们每个人都根据自己的个人经验对时间有一个直观的概念。不幸的是，这种直观的时间概念让我们更容易想象出整体秩序，而不是局部秩序。我们更容易想象出事情一个接一个发生的顺序，而不是同时发生的顺序。推理单一顺序的信息比推理以不同顺序和不同延迟到达的信息更容易。

However, when implementing distributing systems we want to avoid making strong assumptions about time and order, because the stronger the assumptions, the more fragile a system is to issues with the "time sensor" - or the onboard clock. Furthermore, imposing an order carries a cost. The more temporal nondeterminism that we can tolerate, the more we can take advantage of distributed computation.
> 然而，在实现分布式系统时，我们希望避免对时间和顺序做出强有力的假设，因为假设越强，系统就越容易受到 “时间传感器” 或板载时钟的问题的影响。此外，实施命令是有成本的。我们可以容忍的时间不确定性越多，我们就越能利用分布式计算。

### Time with a "global-clock" assumption
The global clock assumption is that there is a global clock of perfect accuracy, and that everyone has access to that clock. This is the way we tend to think about time, because in human interactions small differences in time don't really matter.
> 全局时钟假设是有一个完全准确的全局时钟，并且每个人都可以访问该时钟。 这是我们倾向于思考时间的方式，因为在人类互动中，时间上的微小差异并不重要。
![[global-clock.png]]

The global clock is basically a source of total order (exact order of every operation on all nodes even if those nodes have never communicated).
> 全局时钟基本上是总顺序的来源（所有节点上的每个操作的确切顺序，即使这些节点从未通信过）。

However, this is an idealized view of the world: in reality, clock synchronization is only possible to a limited degree of accuracy. This is limited by the lack of accuracy of clocks in commodity computers, by latency if a clock synchronization protocol such as [NTP](http://en.wikipedia.org/wiki/Network_Time_Protocol) is used and fundamentally by [the nature of spacetime](http://en.wikipedia.org/wiki/Time_dilation).
> 然而，这是一个理想化的世界观：在现实中，时钟同步只能在有限的精度范围内实现。 这受到商品计算机中时钟缺乏准确性的限制，如果使用诸如ntp之类的时钟同步协议，则会受到延迟的限制，并且从根本上受到时空的性质的限制。

Assuming that clocks on distributed nodes are perfectly synchronized means assuming that clocks start at the same value and never drift apart. It's a nice assumption because you can use timestamps freely to determine a global total order - bound by clock drift rather than latency - but this is a nontrivial operational challenge and a potential source of anomalies. There are many different scenarios where a simple failure - such as a user accidentally changing the local time on a machine, or an out-of-date machine joining a cluster, or synchronized clocks drifting at slightly different rates and so on that can cause hard-to-trace anomalies.
> 假设分布式节点上的时钟完全同步意味着假设时钟以相同的值开始并且从不漂移。 这是一个很好的假设，因为您可以自由使用时间戳来确定全局总顺序-受时钟漂移而不是延迟约束-但这是一个非平凡的操作挑战，也是异常的潜在来源。 在许多不同的情况下，一个简单的故障--例如用户意外地更改了计算机上的本地时间，或者一台过时的计算机加入了集群，或者同步时钟以稍微不同的速率漂移，等等，可能导致难以跟踪的异常。

Nevertheless, there are some real-world systems that make this assumption. Facebook's Cassandra is an example of a system that assumes clocks are synchronized. It uses timestamps to resolve conflicts between writes - the write with the newer timestamp wins. This means that if clocks drift, new data may be ignored or overwritten by old data; again, this is an operational challenge (and from what I've heard, one that people are acutely aware of). Another interesting example is Google's Spanner: the paper describes their TrueTime API, which synchronizes time but also estimates worst-case clock drift.
> 尽管如此，还是有一些现实世界的系统做出了这种假设。 Facebook的Cassandra是假设时钟同步的系统的一个例子。 它使用时间戳来解决写入之间的冲突-具有较新时间戳的写入获胜。 这意味着，如果时钟漂移，新数据可能会被忽略或被旧数据复盖;再次，这是一个操作挑战（从我所听到的，人们敏锐地意识到的）。 另一个有趣的例子是谷歌的Spanner：该论文描述了他们的TrueTime API，它同步时间，但也估计了最坏情况的时钟漂移。

### Time with a "Local-clock" assumption
The second, and perhaps more plausible assumption is that each machine has its own clock, but there is no global clock. It means that you cannot use the local clock in order to determine whether a remote timestamp occurred before or after a local timestamp; in other words, you cannot meaningfully compare timestamps from two different machines.
> 第二种假设可能更合理，即每台机器都有自己的时钟，但没有全局时钟。这意味着无法使用本地时钟来确定远程时间戳是发生在本地时间戳之前还是之后；换句话说，无法对来自两台不同机器的时间戳进行有意义的比较。
![[local-clock.png]]

The local clock assumption corresponds more closely to the real world. It assigns a partial order: events on each system are ordered but events cannot be ordered across systems by only using a clock.
> 本地时钟假设与现实世界更接近。 它分配一个部分顺序：每个系统上的事件都是有序的，但仅使用时钟不能跨系统对事件进行排序。

However, you can use timestamps to order events on a single machine; and you can use timeouts on a single machine as long as you are careful not to allow the clock to jump around. Of course, on a machine controlled by an end-user this is probably assuming too much: for example, a user might accidentally change their date to a different value while looking up a date using the operating system's date control.
> 不过，您可以在单台机器上使用时间戳对事件进行排序；您也可以在单台机器上使用超时，只要注意不让时钟跳动即可。当然，在由终端用户控制的机器上，这样做的假设可能过高：例如，用户在使用操作系统的日期控件查找日期时，可能会不小心将日期改成不同的值。

### Time with a "No-clock" assumption
