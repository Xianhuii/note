![[Pasted image 20250119155018.png]]

# 1. Distributed systems at a high level
There are two basic tasks that any computer system needs to accomplish:
- storage and
- computation
> 任何计算机系统都需要完成两项基本任务：存储和计算。

Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers - usually, because the problem no longer fits on a single computer.
> 分布式编程是用多台计算机解决在单台计算机上可以解决的同一问题的艺术--通常是因为该问题不再适合在单台计算机上解决。

However, as problem sizes increase you will reach a point where either the hardware upgrade that allows you to solve the problem on a single node does not exist, or becomes cost-prohibitive. At that point, I welcome you to the world of distributed systems.
> 但是，随着问题规模的增大，你将会遇到这样的情况：要么无法升级硬件，让你在单个节点上解决问题，要么成本过高。这时，我欢迎你来到分布式系统的世界。

It is a current reality that the best value is in mid-range, commodity hardware - as long as the maintenance costs can be kept down through fault-tolerant software.
> 目前的现实情况是，只要能通过容错软件降低维护成本，中端商品硬件的价值最高。

Ideally, adding a new machine would increase the performance and capacity of the system linearly. But of course this is not possible, because there is some overhead that arises due to having separate computers. Data needs to be copied around, computation tasks have to be coordinated and so on. This is why it's worthwhile to study distributed algorithms - they provide efficient solutions to specific problems, as well as guidance about what is possible, what the minimum cost of a correct implementation is, and what is impossible.
> 在理想情况下，增加一台新机器可以线性地提高系统的性能和容量。但这当然是不可能的，因为拥有独立的计算机会产生一些开销。数据需要复制，计算任务需要协调等等。这就是为什么值得研究分布式算法的原因--它们为特定问题提供了高效的解决方案，并指导我们了解哪些是可能的，正确实现的最低成本是多少，以及哪些是不可能的。

## What we want to achieve: Scalability and other good things
[Scalability](http://en.wikipedia.org/wiki/Scalability)：is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth.
> 可扩展性：是指一个系统、网络或流程能够以适当的方式处理不断增长的工作量，或能够扩大以适应这种增长的能力。

- Size scalability: adding more nodes should make the system linearly faster; growing the dataset should not increase latency
- Geographic scalability: it should be possible to use multiple data centers to reduce the time it takes to respond to user queries, while dealing with cross-data center latency in some sensible manner.
- Administrative scalability: adding more nodes should not increase the administrative costs of the system (e.g. the administrators-to-machines ratio).
> 
	规模可扩展性：增加节点应使系统的速度呈线性增长；数据集的增长不应增加延迟
	地理可扩展性：应该可以使用多个数据中心来减少响应用户查询所需的时间，同时以某种合理的方式处理跨数据中心的延迟。
	管理可扩展性：增加节点不应增加系统的管理成本（如管理员与机器之比）。

A scalable system is one that continues to meet the needs of its users as scale increases. There are two particularly relevant aspects - performance and availability - which can be measured in various ways.
>可扩展系统是指随着规模的扩大，仍能满足用户需求的系统。有两个特别相关的方面--性能和可用性--可以通过各种方式来衡量。

### Performance (and latency)
[Performance](http://en.wikipedia.org/wiki/Computer_performance): is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.
> 性能：是指计算机系统完成的有用工作与所用时间和资源的比较。

Depending on the context, this may involve achieving one or more of the following:
- Short response time/low latency for a given piece of work
- High throughput (rate of processing work)
- Low utilization of computing resource(s)
> 根据具体情况，这可能涉及实现以下一个或多个目标：
	特定工作的响应时间短/延迟低
	高吞吐量（处理工作的速度）
	计算资源利用率低

There are tradeoffs involved in optimizing for any of these outcomes. For example, a system may achieve a higher throughput by processing larger batches of work thereby reducing operation overhead. The tradeoff would be longer response times for individual pieces of work due to batching.
> 在对上述任何结果进行优化时，都需要权衡利弊。例如，系统可以通过处理更大批量的工作来获得更高的吞吐量，从而减少运行开销。但这样做的代价是，由于批量处理，单个工作的响应时间会更长。

Latency: The state of being latent; delay, a period between the initiation of something and the occurrence.
> 延迟：潜伏状态；延迟，某事开始和发生之间的一段时间。

Let's assume for a moment that our distributed system does just one high-level task: given a query, it takes all of the data in the system and calculates a single result. In other words, think of a distributed system as a data store with the ability to run a single deterministic computation (function) over its current content:
`result = query(all data in the system)`
> 让我们假设一下，我们的分布式系统只完成一项高级任务：给定一个查询，它获取系统中的所有数据并计算出一个结果。换句话说，可以把分布式系统看作是一个数据存储库，能够对其当前内容进行单次确定性计算（函数）：
	结果 = 查询（系统中的所有数据）

Then, what matters for latency is not the amount of old data, but rather the speed at which new data "takes effect" in the system. For example, latency could be measured in terms of how long it takes for a write to become visible to readers.
> 因此，对延迟而言，重要的不是旧数据的数量，而是新数据在系统中 "生效 "的速度。例如，延迟可以用写入到读者可见所需的时间来衡量。

In a distributed system, there is a minimum latency that cannot be overcome: the speed of light limits how fast information can travel, and hardware components have a minimum latency cost incurred per operation (think RAM and hard drives but also CPUs).
> 在分布式系统中，有一个无法逾越的最低延迟时间：光速限制了信息的传播速度，而硬件组件每次操作都会产生最低延迟成本（例如内存和硬盘，但也包括 CPU）。

How much that minimum latency impacts your queries depends on the nature of those queries and the physical distance the information needs to travel.
> 最小延迟对查询的影响程度取决于查询的性质和信息需要传输的物理距离。

### Availability (and fault tolerance)
[Availability](http://en.wikipedia.org/wiki/High_availability): the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable.
> 可用性：系统处于正常运行状态的时间比例。如果用户无法访问系统，则称系统不可用。

Distributed systems allow us to achieve desirable characteristics that would be hard to accomplish on a single system. For example, a single machine cannot tolerate any failures since it either fails or doesn't.
> 分布式系统使我们能够实现单个系统难以实现的理想特性。例如，单台机器不能容忍任何故障，因为它要么故障，要么不故障。

Systems that have no redundancy can only be as available as their underlying components. Systems built with redundancy can be tolerant of partial failures and thus be more available. It is worth noting that "redundant" can mean different things depending on what you look at - components, servers, datacenters and so on.
> 没有冗余的系统只能像其基础组件一样可用。有冗余的系统可以承受部分故障，因此可用性更高。值得注意的是，"冗余 "可以有不同的含义，这取决于你关注的是什么--组件、服务器、数据中心等。

Formulaically, availability is: `Availability = uptime / (uptime + downtime)`.
> 按公式计算，可用性为可用性 = 正常运行时间/（正常运行时间 + 停机时间）。

Availability from a technical perspective is mostly about being fault tolerant. Because the probability of a failure occurring increases with the number of components, the system should be able to compensate so as to not become less reliable as the number of components increases.
> 从技术角度看，可用性主要是指容错性。由于发生故障的概率会随着组件数量的增加而增加，因此系统应该能够进行补偿，从而不会随着组件数量的增加而降低可靠性。

Availability is in some sense a much wider concept than uptime, since the availability of a service can also be affected by, say, a network outage or the company owning the service going out of business (which would be a factor which is not really relevant to fault tolerance but would still influence the availability of the system). But without knowing every single specific aspect of the system, the best we can do is design for fault tolerance.
> 在某种意义上，可用性是一个比正常运行时间更宽泛的概念，因为服务的可用性也会受到影响，例如网络中断或拥有服务的公司倒闭（这是一个与容错无关的因素，但仍会影响系统的可用性）。但在不了解系统每个具体方面的情况下，我们能做的就是设计容错。

Fault tolerance: ability of a system to behave in a well-defined manner once faults occur.
> 容错：系统在故障发生时以明确定义的方式运行的能力。

Fault tolerance boils down to this: define what faults you expect and then design a system or an algorithm that is tolerant of them. You can't tolerate faults you haven't considered.
> 容错归根结底就是：确定你所期望的故障，然后设计出能够容错的系统或算法。你不可能容忍你没有考虑到的故障。

## What prevents us from achieving good things?
Distributed systems are constrained by two physical factors:
- the number of nodes (which increases with the required storage and computation capacity)
- the distance between nodes (information travels, at best, at the speed of light)
> 分布式系统受到两个物理因素的制约：
	节点数量（随所需存储和计算能力的增加而增加）
	节点之间的距离（信息最多以光速传播）

Working within those constraints:
- an increase in the number of independent nodes increases the probability of failure in a system (reducing availability and increasing administrative costs)
- an increase in the number of independent nodes may increase the need for communication between nodes (reducing performance as scale increases)
- an increase in geographic distance increases the minimum latency for communication between distant nodes (reducing performance for certain operations)
> 在这些限制条件下开展工作：
	增加独立节点的数量会增加系统发生故障的概率（降低可用性并增加管理成本）
	独立节点数量的增加可能会增加节点之间的通信需求（随着规模的扩大而降低性能）
	地理距离的增加会增加远距离节点间通信的最小延迟（降低某些操作的性能）

Beyond these tendencies - which are a result of the physical constraints - is the world of system design options.
> 除了这些由物理限制之外，还有系统设计方案的限制。

Both performance and availability are defined by the external guarantees the system makes. On a high level, you can think of the guarantees as the SLA (service level agreement) for the system: if I write data, how quickly can I access it elsewhere? After the data is written, what guarantees do I have of durability? If I ask the system to run a computation, how quickly will it return results? When components fail, or are taken out of operation, what impact will this have on the system?
> 性能和可用性都是由系统的外部保证来定义的。从高层次上讲，可以将这些保证视为系统的 SLA（服务水平协议）：如果我写入数据，我在其他地方访问数据的速度有多快？数据写入后，我如何保证数据的持久性？如果我要求系统运行计算，它能多快返回结果？当组件发生故障或停止运行时，会对系统产生什么影响？

There is another criterion, which is not explicitly mentioned but implied: intelligibility. How understandable are the guarantees that are made? Of course, there are no simple metrics for what is intelligible.
> 还有一个标准没有明确提及，但隐含其中：可理解性。所做的保证有多容易理解？当然，什么是可理解性并没有简单的衡量标准。

## Abstractions and models
This is where abstractions and models come into play. Abstractions make things more manageable by removing real-world aspects that are not relevant to solving a problem. Models describe the key properties of a distributed system in a precise manner. I'll discuss many kinds of models in the next chapter, such as:
- System model (asynchronous / synchronous)
- Failure model (crash-fail, partitions, Byzantine)
- Consistency model (strong, eventual)
> 这就是抽象和模型发挥作用的地方。抽象可以去除现实世界中与解决问题无关的方面，从而使问题更易于管理。模型以精确的方式描述了分布式系统的关键属性。我将在下一章讨论多种模型，如
	系统模型（异步/同步）
	故障模型（崩溃-故障、分区、拜占庭）
	一致性模型（强一致性、最终一致性）

A good abstraction makes working with a system easier to understand, while capturing the factors that are relevant for a particular purpose.
> 一个好的抽象概念可以使系统的工作更容易理解，同时还能捕捉到与特定目的相关的因素。

## Design techniques: partition and replicate
The manner in which a data set is distributed between multiple nodes is very important. In order for any computation to happen, we need to locate the data and then act on it.
> 数据集在多个节点之间分布的方式非常重要。为了进行计算，我们需要找到数据，然后对其进行操作。

There are two basic techniques that can be applied to a data set. It can be split over multiple nodes (partitioning) to allow for more parallel processing. It can also be copied or cached on different nodes to reduce the distance between the client and the server and for greater fault tolerance (replication).
> 有两种基本技术可用于数据集。可以将数据分割到多个节点上（分区），以便进行更多并行处理。也可以在不同的节点上复制或缓存数据，以缩短客户端与服务器之间的距离，提高容错能力（复制）。

The picture below illustrates the difference between these two: partitioned data (A and B below) is divided into independent sets, while replicated data (C below) is copied to multiple locations.
> 下图说明了两者之间的区别：分区数据（下图中的 A 和 B）被划分为独立的数据集，而复制数据（下图中的 C）被复制到多个位置。
![[part-repl.png]]

This is the one-two punch for solving any problem where distributed computing plays a role. Of course, the trick is in picking the right technique for your concrete implementation; there are many algorithms that implement replication and partitioning, each with different limitations and advantages which need to be assessed against your design objectives.
> 这是解决任何涉及分布式计算问题的一记重拳。当然，诀窍在于为您的具体实现选择正确的技术；有许多实现复制和分区的算法，每种算法都有不同的局限性和优势，需要根据您的设计目标进行评估。

### Partitioning
Partitioning is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data.
- Partitioning improves performance by limiting the amount of data to be examined and by locating related data in the same partition
- Partitioning improves availability by allowing partitions to fail independently, increasing the number of nodes that need to fail before availability is sacrificed
> 分区是指将数据集划分为不同的独立小集；这用于减少数据集增长的影响，因为每个分区都是数据的一个子集。
	分区可以限制需要检查的数据量，并将相关数据定位在同一分区中，从而提高性能。
	分区允许分区独立发生故障，增加了在牺牲可用性之前需要发生故障的节点数量，从而提高了可用性

Partitioning is also very much application-specific, so it is hard to say much about it without knowing the specifics. That's why the focus is on replication in most texts, including this one.
> 分区在很大程度上也与具体应用有关，因此，在不了解具体情况的情况下，很难对其进行深入探讨。这就是为什么大多数文章（包括本文章）的重点都放在复制上。

### Replication
Replication is making copies of the same data on multiple machines; this allows more servers to take part in the computation.
> 复制是指在多台机器上复制相同的数据；这样可以让更多服务器参与计算。

Replication - copying or reproducing something - is the primary way in which we can fight latency.
- Replication improves performance by making additional computing power and bandwidth applicable to a new copy of the data
- Replication improves availability by creating additional copies of the data, increasing the number of nodes that need to fail before availability is sacrificed
> 复制 - 复制或复制某些东西 - 是我们对抗延迟的主要方式。
	复制通过使额外的计算能力和带宽适用于新的数据拷贝来提高性能
	复制通过创建额外的数据副本来提高可用性，从而增加在牺牲可用性之前需要失败的节点数

Replication is about providing extra bandwidth, and caching where it counts. It is also about maintaining consistency in some way according to some consistency model.
> 复制是为了提供额外的带宽，并在重要的地方进行缓存。此外，复制还需要根据某种一致性模型，以某种方式保持一致性。

Replication allows us to achieve scalability, performance and fault tolerance. Afraid of loss of availability or reduced performance? Replicate the data to avoid a bottleneck or single point of failure. Slow computation? Replicate the computation on multiple systems. Slow I/O? Replicate the data to a local cache to reduce latency or onto multiple machines to increase throughput.
> 复制使我们能够实现可扩展性、性能和容错。害怕丧失可用性或降低性能？复制数据以避免瓶颈或单点故障。计算速度慢？在多个系统上复制计算。I/O 速度慢？将数据复制到本地缓存以减少延迟，或复制到多台机器上以提高吞吐量。

Replication is also the source of many of the problems, since there are now independent copies of the data that has to be kept in sync on multiple machines - this means ensuring that the replication follows a consistency model.
> 复制也是许多问题的根源，因为现在有独立的数据副本，必须在多台机器上保持同步--这意味着要确保复制遵循一致性模型。

The choice of a consistency model is crucial: a good consistency model provides clean semantics for programmers (in other words, the properties it guarantees are easy to reason about) and meets business/design goals such as high availability or strong consistency.
> 一致性模型的选择至关重要：一个好的一致性模型能为程序员提供简洁的语义（换句话说，它所保证的属性易于推理），并能满足业务/设计目标，如高可用性或强一致性。

Only one consistency model for replication - strong consistency - allows you to program as-if the underlying data was not replicated. Other consistency models expose some internals of the replication to the programmer. However, weaker consistency models can provide lower latency and higher availability - and are not necessarily harder to understand, just different.
> 只有一种复制的一致性模型--强一致性--允许你在编程时假设底层数据没有被复制。其他一致性模型会向程序员暴露复制的一些内部信息。然而，较弱的一致性模型可以提供更低的延迟和更高的可用性，而且并不一定更难理解，只是不同而已。

# 2. Up and down the level of abstraction
## A system model
A key property of distributed systems is distribution. More specifically, programs in a distributed system:
- run concurrently on independent nodes ...
- are connected by a network that may introduce nondeterminism and message loss ...
- and have no shared memory or shared clock.
> 分布式系统的一个关键特性是分布。更具体地说，分布式系统中的程序：
	在独立节点上并发运行...
	由网络连接，可能引入非确定性和信息丢失...
	没有共享内存或共享时钟。

There are many implications:
- each node executes a program concurrently
- knowledge is local: nodes have fast access only to their local state, and any information about global state is potentially out of date
- nodes can fail and recover from failure independently
- messages can be delayed or lost (independent of node failure; it is not easy to distinguish network failure and node failure)
- and clocks are not synchronized across nodes (local timestamps do not correspond to the global real time order, which cannot be easily observed)
> 这有很多影响：
	每个节点同时执行一个程序
	知识是本地的：节点只能快速访问其本地状态，任何有关全局状态的信息都可能是过时的
	节点可以独立发生故障并从故障中恢复
	信息可能会延迟或丢失（与节点故障无关；不易区分网络故障和节点故障）
	节点间的时钟不同步（本地时间戳与全局实时顺序不一致，不易观察到）

A system model enumerates the many assumptions associated with a particular system design.
> 系统模型列举了与特定系统设计相关的许多假设。

System model: a set of assumptions about the environment and facilities on which a distributed system is implemented
> 系统模型：关于实施分布式系统的环境和设施的一系列假设

System models vary in their assumptions about the environment and facilities. These assumptions include:
- what capabilities the nodes have and how they may fail
- how communication links operate and how they may fail and
- properties of the overall system, such as assumptions about time and order
> 系统模型对环境和设施的假设各不相同。这些假设包括
	节点具有哪些能力，它们可能如何失效
	通信链路如何运行以及可能出现的故障
	整个系统的属性，如对时间和顺序的假设

A robust system model is one that makes the weakest assumptions: any algorithm written for such a system is very tolerant of different environments, since it makes very few and very weak assumptions.
> 稳健的系统模型是一种假设最弱的模型：为这种系统编写的任何算法都能很好地容忍不同的环境，因为它的假设非常少而且非常弱。

On the other hand, we can create a system model that is easy to reason about by making strong assumptions. For example, assuming that nodes do not fail means that our algorithm does not need to handle node failures. However, such a system model is unrealistic and hence hard to apply into practice.
> 另一方面，我们可以通过做出强有力的假设来创建一个易于推理的系统模型。例如，假设节点不会发生故障，就意味着我们的算法无需处理节点故障。然而，这样的系统模型是不现实的，因此很难应用到实践中。

### Nodes in our system model
Nodes serve as hosts for computation and storage. They have:
- the ability to execute a program
- the ability to store data into volatile memory (which can be lost upon failure) and into stable state (which can be read after a failure)
- a clock (which may or may not be assumed to be accurate)
> 节点是计算和存储的主机。它们具有
	执行程序的能力
	将数据存储到易失性存储器（故障时可能丢失）和稳定状态（故障后可读取）的能力
	时钟（可能准确，也可能不准确）

Nodes execute deterministic algorithms: the local computation, the local state after the computation, and the messages sent are determined uniquely by the message received and local state when the message was received.
> 节点执行确定性算法：本地计算、计算后的本地状态和发送的信息由收到的信息和收到信息时的本地状态唯一决定。

There are many possible failure models which describe the ways in which nodes can fail. In practice, most systems assume a crash-recovery failure model: that is, nodes can only fail by crashing, and can (possibly) recover after crashing at some later point.
> 有许多可能的故障模型可以描述节点可能发生故障的方式。在实践中，大多数系统都假设了崩溃-恢复故障模型：也就是说，节点只能通过崩溃来发生故障，并（可能）在崩溃后的某个时间点恢复。

Another alternative is to assume that nodes can fail by misbehaving in any arbitrary way. This is known as [Byzantine fault tolerance](http://en.wikipedia.org/wiki/Byzantine_fault_tolerance). Byzantine faults are rarely handled in real world commercial systems, because algorithms resilient to arbitrary faults are more expensive to run and more complex to implement. I will not discuss them here.
> 另一种方法是假定节点可能以任意方式行为不当而发生故障。这就是所谓的拜占庭容错。在现实世界的商业系统中，拜占庭故障很少得到处理，因为能够抵御任意故障的算法运行成本更高，实施起来也更复杂。在此，我将不作讨论。

### Communication links in our system model
Communication links connect individual nodes to each other, and allow messages to be sent in either direction. Many books that discuss distributed algorithms assume that there are individual links between each pair of nodes, that the links provide FIFO (first in, first out) order for messages, that they can only deliver messages that were sent, and that sent messages can be lost.
> 通信链路将单个节点相互连接起来，并允许向任一方向发送信息。许多讨论分布式算法的书籍都假定，每对节点之间都有单独的链路，链路为信息提供 FIFO（先进先出）顺序，链路只能传递已发送的信息，而且已发送的信息可能会丢失。

Some algorithms assume that the network is reliable: that messages are never lost and never delayed indefinitely. This may be a reasonable assumption for some real-world settings, but in general it is preferable to consider the network to be unreliable and subject to message loss and delays.
> 有些算法假设网络是可靠的：信息不会丢失，也不会无限期延迟。对于某些实际情况来说，这种假设可能是合理的，但一般来说，最好还是将网络视为不可靠的，会出现信息丢失和延迟。

A network partition occurs when the network fails while the nodes themselves remain operational. When this occurs, messages may be lost or delayed until the network partition is repaired. Partitioned nodes may be accessible by some clients, and so must be treated differently from crashed nodes. The diagram below illustrates a node failure vs. a network partition:
> 当网络发生故障，而节点本身仍可正常运行时，就会出现网络分区。发生这种情况时，信息可能会丢失或延迟，直到网络分区被修复。被分区的节点可能会被某些客户端访问，因此必须与崩溃的节点区别对待。下图展示了节点故障与网络分区的对比：
![[system-of-2.png]]

It is rare to make further assumptions about communication links. We could assume that links only work in one direction, or we could introduce different communication costs (e.g. latency due to physical distance) for different links. However, these are rarely concerns in commercial environments except for long-distance links (WAN latency) and so I will not discuss them here; a more detailed model of costs and topology allows for better optimization at the cost of complexity.
> 对通信链路做进一步的假设是很少见的。我们可以假设链路只在一个方向上工作，也可以为不同的链路引入不同的通信成本（如物理距离造成的延迟）。不过，除了长距离链接（广域网延迟）外，商业环境中很少涉及这些问题，因此我在此不作讨论；更详细的成本和拓扑模型可以在降低复杂性的同时实现更好的优化。

### Timing / ordering assumptions
One of the consequences of physical distribution is that each node experiences the world in a unique manner. This is inescapable, because information can only travel at the speed of light. If nodes are at different distances from each other, then any messages sent from one node to the others will arrive at a different time and potentially in a different order at the other nodes.
> 物理分布的后果之一是，每个节点都以独特的方式体验世界。这是不可避免的，因为信息只能以光速传播。如果节点之间的距离不同，那么从一个节点发送到其他节点的任何信息都会在不同的时间到达其他节点，并且可能以不同的顺序到达。

Timing assumptions are a convenient shorthand for capturing assumptions about the extent to which we take this reality into account. The two main alternatives are:
- Synchronous system model: Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock
- Asynchronous system model: No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist
> 时间假设是一种方便的简写，用来描述我们在多大程度上考虑到这一现实的假设。两种主要的选择是
	同步系统模型：进程同步执行；信息传输延迟有一个已知的上限；每个进程都有一个准确的时钟
	异步系统模型：没有时序假设--例如，进程以独立的速率执行；信息传输延迟没有上限；不存在有用的时钟

The synchronous system model imposes many constraints on time and order. It essentially assumes that the nodes have the same experience: that messages that are sent are always received within a particular maximum transmission delay, and that processes execute in lock-step. This is convenient, because it allows you as the system designer to make assumptions about time and order, while the asynchronous system model doesn't.
> 同步系统模型对时间和顺序施加了许多限制。它基本上假定节点具有相同的经验：发送的信息总是在特定的最大传输延迟内收到，并且进程按部就班地执行。这很方便，因为它允许系统设计者对时间和顺序做出假设，而异步系统模型则不允许。

Asynchronicity is a non-assumption: it just assumes that you can't rely on timing (or a "time sensor").
> 异步性是一个非假设：它只是假设你不能依赖计时（或“时间传感器”）。

It is easier to solve problems in the synchronous system model, because assumptions about execution speeds, maximum message transmission delays and clock accuracy all help in solving problems since you can make inferences based on those assumptions and rule out inconvenient failure scenarios by assuming they never occur.
> 在同步系统模型中更容易解决问题，因为对执行速度、最大信息传输延迟和时钟精度的假设都有助于解决问题，因为你可以根据这些假设做出推断，并通过假设它们永远不会发生来排除不方便的故障情况。

Of course, assuming the synchronous system model is not particularly realistic. Real-world networks are subject to failures and there are no hard bounds on message delay. Real world systems are at best partially synchronous: they may occasionally work correctly and provide some upper bounds, but there will be times where messages are delayed indefinitely and clocks are out of sync. I won't really discuss algorithms for synchronous systems here, but you will probably run into them in many other introductory books because they are analytically easier (but unrealistic).
> 当然，假设同步系统模型并不特别现实。现实世界的网络可能会出现故障，而且信息延迟也没有硬约束。现实世界中的系统充其量只是部分同步：它们偶尔会正常工作并提供一些上限，但有时也会出现信息无限期延迟和时钟不同步的情况。我不会在这里讨论同步系统的算法，但你可能会在许多其他入门书籍中看到它们，因为它们在分析上更容易（但不现实）。

### The consensus problem 共识问题
Several computers (or nodes) achieve consensus if they all agree on some value. More formally:
1. Agreement: Every correct process must agree on the same value.
2. Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.
3. Termination: All processes eventually reach a decision.
4. Validity: If all correct processes propose the same value V, then all correct processes decide V.
> 如果多台计算机（或节点）都同意某个值，就能达成共识。更正式的说法是
	协议：每个正确的进程都必须就相同的值达成一致。
	完整性：每个正确的进程最多决定一个值，如果它决定了某个值，那么它一定是由某个进程提出的。
	终止：所有进程最终都会做出决定。
	有效性：如果所有正确的进程都提出了相同的值 V，那么所有正确的进程都会决定 V。

The consensus problem is at the core of many commercial distributed systems. After all, we want the reliability and performance of a distributed system without having to deal with the consequences of distribution (e.g. disagreements / divergence between nodes), and solving the consensus problem makes it possible to solve several related, more advanced problems such as atomic broadcast and atomic commit.
> 共识问题是许多商业分布式系统的核心问题。毕竟，我们想要的是分布式系统的可靠性和性能，而无需处理分布式的后果（如节点间的分歧/分歧），解决了共识问题，就有可能解决几个相关的、更高级的问题，如原子广播和原子提交。

### Two impossibility results
The first impossibility result, known as the FLP impossibility result, is an impossibility result that is particularly relevant to people who design distributed algorithms. The second - the CAP theorem - is a related result that is more relevant to practitioners; people who need to choose between different system designs but who are not directly concerned with the design of algorithms.
> 第一个不可能结果被称为 FLP 不可能结果，是一个与设计分布式算法的人特别相关的不可能结果。第二个不可能结果--CAP定理--是一个与实践者更相关的结果；实践者需要在不同的系统设计之间做出选择，但他们并不直接关注算法的设计。

## The FLP impossibility result
The FLP impossibility result (named after the authors, Fischer, Lynch and Patterson) examines the consensus problem under the asynchronous system model (technically, the agreement problem, which is a very weak form of the consensus problem). It is assumed that nodes can only fail by crashing; that the network is reliable, and that the typical timing assumptions of the asynchronous system model hold: e.g. there are no bounds on message delay.
> FLP 不可能性结果（以作者费舍尔、林奇和帕特森的名字命名）研究的是异步系统模型下的共识问题（严格来说是协议问题，是共识问题的一种非常弱的形式）。假设节点只能以崩溃的方式失败；网络是可靠的；异步系统模型的典型时序假设成立：例如，消息延迟不受约束。

Under these assumptions, the FLP result states that "there does not exist a (deterministic) algorithm for the consensus problem in an asynchronous system subject to failures, even if messages can never be lost, at most one process may fail, and it can only fail by crashing (stopping executing)".
> 在这些假设条件下，FLP 结果表明："在异步通信模型下，一个分布式系统中，即使只有一个进程可能会出现故障，也不存在一个完全正确的、确定性的算法来达成一致性"。

This result means that there is no way to solve the consensus problem under a very minimal system model in a way that cannot be delayed forever. The argument is that if such an algorithm existed, then one could devise an execution of that algorithm in which it would remain undecided ("bivalent") for an arbitrary amount of time by delaying message delivery - which is allowed in the asynchronous system model. Thus, such an algorithm cannot exist.
> 这一结果意味着，在一个极小的系统模型下，没有办法以一种不能永远延迟的方式来解决共识问题。我们的论点是，如果存在这样一种算法，那么我们就可以设计出一种算法的执行方式，通过延迟消息传递（这在异步系统模型中是允许的），在任意长的时间内保持未决（"二价"）。因此，这样的算法是不可能存在的。

This impossibility result is important because it highlights that assuming the asynchronous system model leads to a tradeoff: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold.
> 这一不可能性结果非常重要，因为它突出表明，假设采用异步系统模型会导致一种权衡：当有关消息传递界限的保证不成立时，解决共识问题的算法必须要么放弃安全性，要么放弃有效性。

This insight is particularly relevant to people who design algorithms, because it imposes a hard constraint on the problems that we know are solvable in the asynchronous system model. The CAP theorem is a related theorem that is more relevant to practitioners: it makes slightly different assumptions (network failures rather than node failures), and has more clear implications for practitioners choosing between system designs.
> 这一见解与设计算法的人员尤为相关，因为它对我们已知在异步系统模型中可以解决的问题施加了一个硬约束。CAP 定理是一个与实践者更相关的相关定理：它的假设略有不同（是网络故障而不是节点故障），对实践者选择系统设计有更明确的影响。

## The CAP theorem
The theorem states that of these three properties:
- Consistency: all nodes see the same data at the same time.
- Availability: node failures do not prevent survivors from continuing to operate.
- Partition tolerance: the system continues to operate despite message loss due to network and/or node failure
> 该定理指出，在这三个特性中：
	一致性：所有节点在同一时间看到相同的数据。
	可用性：节点故障不会妨碍幸存者继续运行。
	分区容忍性：尽管网络和/或节点故障导致信息丢失，系统仍能继续运行

only two can be satisfied simultaneously. We can even draw this as a pretty diagram, picking two properties out of three gives us three types of systems that correspond to different intersections:
> 只有两个可以同时满足。我们甚至可以把它画成一张漂亮的图，从三个属性中选出两个，就可以得到三种类型的系统，它们对应着不同的交叉点：
![[CAP.png]]

Note that the theorem states that the middle piece (having all three properties) is not achievable. Then we get three different system types:
- CA (consistency + availability). Examples include full strict quorum protocols, such as two-phase commit.
- CP (consistency + partition tolerance). Examples include majority quorum protocols in which minority partitions are unavailable such as Paxos.
- AP (availability + partition tolerance). Examples include protocols using conflict resolution, such as Dynamo.
> 请注意，该定理说明中间部分（具备所有三个属性）是无法实现的。这样，我们就得到了三种不同的系统类型：
	CA（一致性 + 可用性）。例如完全严格的法定人数协议，如两阶段提交。
	CP（一致性 + 分区容错）。例如 Paxos 等少数分区不可用的多数法定人数协议。
	AP（可用性 + 分区容错）。例子包括使用冲突解决的协议，如 Dynamo。

The CA and CP system designs both offer the same consistency model: strong consistency. The only difference is that a CA system cannot tolerate any node failures; a CP system can tolerate up to `f` faults given `2f+1` nodes in a non-Byzantine failure model (in other words, it can tolerate the failure of a minority `f` of the nodes as long as majority `f+1` stays up). The reason is simple:
- A CA system does not distinguish between node failures and network failures, and hence must stop accepting writes everywhere to avoid introducing divergence (multiple copies). It cannot tell whether a remote node is down, or whether just the network connection is down: so the only safe thing is to stop accepting writes.
- A CP system prevents divergence (e.g. maintains single-copy consistency) by forcing asymmetric behavior on the two sides of the partition. It only keeps the majority partition around, and requires the minority partition to become unavailable (e.g. stop accepting writes), which retains a degree of availability (the majority partition) and still ensures single-copy consistency.
> CA 和 CP 系统设计都提供相同的一致性模型：强一致性。唯一不同的是，CA 系统不能容忍任何节点故障；而 CP 系统在非拜占庭故障模型中，在 2f+1 个节点的情况下，最多可容忍 f 个故障（换句话说，只要多数 f+1 保持正常，它就能容忍少数 f 个节点的故障）。原因很简单：
	CA 系统无法区分节点故障和网络故障，因此必须停止接受任何地方的写入，以避免引入分歧（多副本）。它无法分辨是远程节点宕机，还是仅仅是网络连接宕机：因此唯一安全的办法就是停止接受写入。
	CP 系统通过强制分区两侧的非对称行为来防止分歧（例如，保持单副本一致性）。它只保留多数分区，而要求少数分区变得不可用（例如停止接受写入），这样就保留了一定程度的可用性（多数分区），并仍能确保单拷贝一致性。

The important thing is that CP systems incorporate network partitions into their failure model and distinguish between a majority partition and a minority partition using an algorithm like Paxos, Raft or viewstamped replication. CA systems are not partition-aware, and are historically more common: they often use the two-phase commit algorithm and are common in traditional distributed relational databases.
> 重要的是，CP 系统将网络分区纳入其故障模型，并使用 Paxos、Raft 或视图戳复制等算法区分多数分区和少数分区。CA 系统没有分区意识，在历史上比较常见：它们通常使用两阶段提交算法，在传统的分布式关系数据库中很常见。

Assuming that a partition occurs, the theorem reduces to a binary choice between availability and consistency.
> 假设发生了分区，该定理就简化为可用性和一致性之间的二元选择。
![[CAP_choice.png]]

I think there are four conclusions that should be drawn from the CAP theorem:
- First, that many system designs used in early distributed relational database systems did not take into account partition tolerance (e.g. they were CA designs). Partition tolerance is an important property for modern systems, since network partitions become much more likely if the system is geographically distributed (as many large systems are).
- Second, that there is a tension between strong consistency and high availability during network partitions. The CAP theorem is an illustration of the tradeoffs that occur between strong guarantees and distributed computation.
- Third, that _there is a tension between strong consistency and performance in normal operation_.
- Fourth - and somewhat indirectly - that _if we do not want to give up availability during a network partition, then we need to explore whether consistency models other than strong consistency are workable for our purposes_.
> 我认为应该从 CAP 定理中得出四个结论：
	首先，早期分布式关系数据库系统中使用的许多系统设计都没有考虑分区容错（例如，它们是 CA 设计）。分区容错是现代系统的一个重要特性，因为如果系统在地理上是分布式的（许多大型系统都是如此），网络分区的可能性就会大大增加。
	其次，在网络分区期间，强一致性和高可用性之间存在矛盾。CAP 定理说明了强保证和分布式计算之间的权衡。
	第三，高度一致性与正常运行性能之间存在矛盾。
	第四--有点间接--如果我们不想在网络分区期间放弃可用性，那么我们就需要探索强一致性以外的一致性模型是否能满足我们的目的。

Consistency and availability are not really binary choices, unless you limit yourself to strong consistency. But strong consistency is just one consistency model: the one where you, by necessity, need to give up availability in order to prevent more than a single copy of the data from being active. As Brewer himself points out, the "2 out of 3" interpretation is misleading.
> 一致性和可用性其实并不是二元选择，除非你把自己限制在强一致性上。但强一致性只是一种一致性模型：在这种模型中，你必须放弃可用性，以防止数据的单个副本以上处于活动状态。正如布鲁尔自己所指出的，"三选二 "的解释是有误导性的。

Consistency model: a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable
> 一致性模型：程序员和系统之间的契约，其中系统保证如果程序员遵循某些特定规则，对数据存储的操作结果将是可预测的

The "C" in CAP is "strong consistency", but "consistency" is not a synonym for "strong consistency".
> CAP 中的 "C "是 "强一致性"，但 "一致性 "并不是 "强一致性 "的同义词。

## Strong consistency vs. other consistency models
Consistency models can be categorized into two types: strong and weak consistency models:
- Strong consistency models (capable of maintaining a single copy)
    - Linearizable consistency
    - Sequential consistency
- Weak consistency models (not strong)
    - Client-centric consistency models
    - Causal consistency: strongest model available
    - Eventual consistency models
> 一致性模型可分为两类：强一致性模型和弱一致性模型：
	强一致性模型（能够保持单一副本）
		可线性化的一致性
		顺序一致性
	弱一致性模型（不强）
		以客户为中心的一致性模型
		因果一致性：现有最强模型
		最终一致性模型

Strong consistency models guarantee that the apparent order and visibility of updates is equivalent to a non-replicated system. Weak consistency models, on the other hand, do not make such guarantees.
> 强一致性模型保证更新的表观顺序和可见性等同于非复制系统。而弱一致性模型则没有这样的保证。

### Strong consistency models
Strong consistency models can further be divided into two similar, but slightly different consistency models:
- _Linearizable consistency_: Under linearizable consistency, all operations **appear** to have executed atomically in an order that is consistent with the global real-time ordering of operations. (Herlihy & Wing, 1991)
- _Sequential consistency_: Under sequential consistency, all operations **appear** to have executed atomically in some order that is consistent with the order seen at individual nodes and that is equal at all nodes. (Lamport, 1979)
> 强一致性模型又可分为两种相似但略有不同的一致性模型：
	可线性化一致性：在可线性化一致性下，所有操作似乎都以原子方式执行，其顺序与操作的全局实时排序一致。(Herlihy & Wing, 1991）
	顺序一致性：在顺序一致性条件下，所有操作似乎都是按原子顺序执行的，这种顺序与单个节点上的顺序一致，并且在所有节点上都相同。(Lamport, 1979）

The key difference is that linearizable consistency requires that the order in which operations take effect is equal to the actual real-time ordering of operations. Sequential consistency allows for operations to be reordered as long as the order observed on each node remains consistent. The only way someone can distinguish between the two is if they can observe all the inputs and timings going into the system; from the perspective of a client interacting with a node, the two are equivalent.
> 主要区别在于，可线性化一致性要求操作生效的顺序与操作的实际实时顺序相同。顺序一致性允许对操作重新排序，只要在每个节点上观察到的顺序保持一致即可。区分这两者的唯一方法就是观察进入系统的所有输入和时序；从客户端与节点交互的角度来看，两者是等价的。

### Client-centric consistency models
_Client-centric consistency models_ are consistency models that involve the notion of a client or session in some way. For example, a client-centric consistency model might guarantee that a client will never see older versions of a data item. This is often implemented by building additional caching into the client library, so that if a client moves to a replica node that contains old data, then the client library returns its cached value rather than the old value from the replica.
> 以客户为中心的一致性模型是以某种方式涉及客户或会话概念的一致性模型。例如，以客户端为中心的一致性模型可以保证客户端永远不会看到数据项的旧版本。这通常是通过在客户端库中建立额外的缓存来实现的，这样，如果客户端移动到包含旧数据的副本节点，客户端库就会返回其缓存值，而不是副本中的旧值。

Clients may still see older versions of the data, if the replica node they are on does not contain the latest version, but they will never see anomalies where an older version of a value resurfaces (e.g. because they connected to a different replica). Note that there are many kinds of consistency models that are client-centric.
> 如果客户端所在的副本节点不包含最新版本的数据，客户端可能仍会看到旧版本的数据，但绝不会看到旧版本值再次出现的异常情况（例如，因为客户端连接到了不同的副本）。请注意，以客户端为中心的一致性模型有很多种。

### Eventual consistency
The _eventual consistency_ model says that if you stop changing values, then after some undefined amount of time all replicas will agree on the same value. It is implied that before that time results between replicas are inconsistent in some undefined manner. Since it is [trivially satisfiable](http://www.bailis.org/blog/safety-and-liveness-eventual-consistency-is-not-safe/) (liveness property only), it is useless without supplemental information.
> 最终一致性模型认为，如果停止更改值，那么在某段未定义的时间后，所有副本的值将一致。这意味着在此之前，各副本之间的结果会以某种未定义的方式不一致。由于它是微不足道的可满足性（仅有效性属性），因此在没有补充信息的情况下，它是无用的。

Saying something is merely eventually consistent is like saying "people are eventually dead". It's a very weak constraint, and we'd probably want to have at least some more specific characterization of two things:
First, how long is "eventually"? It would be useful to have a strict lower bound, or at least some idea of how long it typically takes for the system to converge to the same value.
Second, how do the replicas agree on a value? A system that always returns "42" is eventually consistent: all replicas agree on the same value. It just doesn't converge to a useful value since it just keeps returning the same fixed value. Instead, we'd like to have a better idea of the method. For example, one way to decide is to have the value with the largest timestamp always win.
> 说某件事情最终是一致的，就好比说 "人最终会死"。这是一个非常薄弱的约束，我们可能希望至少对两件事有一些更具体的描述：
	首先，"最终 "是多久？如果能有一个严格的下限，或者至少知道系统收敛到相同值一般需要多长时间，那将是非常有用的。
	其次，复制如何就某个值达成一致？一个总是返回 "42 "的系统最终是一致的：所有副本都同意相同的值。但它并没有收敛到一个有用的值，因为它只是不断返回相同的固定值。相反，我们希望对方法有更好的认识。例如，一种决定方法是让时间戳最大的值总是胜出。

So when vendors say "eventual consistency", what they mean is some more precise term, such as "eventually last-writer-wins, and read-the-latest-observed-value in the meantime" consistency. The "how?" matters, because a bad method can lead to writes being lost - for example, if the clock on one node is set incorrectly and timestamps are used.
> 因此，当供应商说 "最终一致性 "时，他们指的是一些更精确的术语，比如 "最终最后写入者获胜，同时读取最新观测值 "的一致性。怎么做？"很重要，因为一个糟糕的方法可能会导致写入丢失--例如，如果一个节点上的时钟设置错误并使用了时间戳。

# 3. Time and order
Any system that can only do one thing at a time will create a total order of operations. Like people passing through a single door, every operation will have a well-defined predecessor and successor. That's basically the programming model that we've worked very hard to preserve.
> 任何一次只能做一件事的系统都会产生一个总的操作顺序。就像人们通过一扇门一样，每个操作都有明确的前置和后继操作。这基本上就是我们一直努力维护的编程模型。

The traditional model is: a single program, one process, one memory space running on one CPU. The operating system abstracts away the fact that there might be multiple CPUs and multiple programs, and that the memory on the computer is actually shared among many programs. I'm not saying that threaded programming and event-oriented programming don't exist; it's just that they are special abstractions on top of the "one/one/one" model. Programs are written to be executed in an ordered fashion: you start from the top, and then go down towards the bottom.
> 传统模式是：一个程序、一个进程、一个内存空间在一个 CPU 上运行。操作系统抽象掉了可能存在多个 CPU 和多个程序的事实，也抽象掉了计算机上的内存实际上是由许多程序共享的事实。我并不是说线程编程和面向事件编程不存在，只是它们是 "一/一/一 "模型之上的特殊抽象。程序的编写是为了以有序的方式执行：从顶层开始，然后向下执行。

Order as a property has received so much attention because the easiest way to define "correctness" is to say "it works like it would on a single machine". And that usually means that a) we run the same operations and b) that we run them in the same order - even if there are multiple machines.
> 顺序作为一种属性受到如此多的关注，是因为定义 "正确性 "的最简单方法就是说 "它能像在单台机器上一样运行"。这通常意味着：a）我们运行相同的操作；b）我们以相同的顺序运行这些操作--即使有多台机器。

The nice thing about distributed systems that preserve order (as defined for a single system) is that they are generic. You don't need to care about what the operations are, because they will be executed exactly like on a single machine. This is great because you know that you can use the same system no matter what the operations are.
> 分布式系统之所以能保持秩序（如单个系统所定义的那样），是因为它们是通用的。你不需要关心操作是什么，因为它们的执行方式与单台机器上的完全一样。这一点非常好，因为你知道，无论操作是什么，你都可以使用同一个系统。

In reality, a distributed program runs on multiple nodes; with multiple CPUs and multiple streams of operations coming in. You can still assign a total order, but it requires either accurate clocks or some form of communication. You could timestamp each operation using a completely accurate clock then use that to figure out the total order. Or you might have some kind of communication system that makes it possible to assign sequential numbers as in a total order.
> 实际上，分布式程序运行在多个节点上，有多个 CPU 和多个操作流。你仍然可以分配总顺序，但这需要精确的时钟或某种形式的通信。你可以用一个完全精确的时钟为每个操作打上时间戳，然后用它来计算总顺序。或者你也可以使用某种通信系统来分配总顺序中的顺序号。

## Total and partial order
The natural state in a distributed system is [partial order](http://en.wikipedia.org/wiki/Partially_ordered_set). Neither the network nor independent nodes make any guarantees about relative order; but at each node, you can observe a local order.
> 分布式系统的自然状态是局部秩序。网络和独立节点都不保证相对秩序；但在每个节点上，你都能观察到局部秩序。

A [total order](http://en.wikipedia.org/wiki/Total_order) is a binary relation that defines an order for every element in some set.
> 总序是一种二元关系，它定义了某个集合中每个元素的顺序。

In a system consisting of one node, a total order emerges by necessity: instructions are executed and messages are processed in a specific, observable order in a single program. We've come to rely on this total order - it makes executions of programs predictable. This order can be maintained on a distributed system, but at a cost: communication is expensive, and time synchronization is difficult and fragile.
> 在一个由单个节点组成的系统中，必然会出现一种总体秩序：在单个程序中，指令以特定的、可观察到的顺序执行，信息以特定的、可观察到的顺序处理。我们已经开始依赖这种总体秩序--它使程序的执行具有可预测性。这种秩序可以在分布式系统中保持，但代价是：通信费用昂贵，时间同步困难且脆弱。

# What is time?
Timestamps really are a shorthand value for representing the state of the world from the start of the universe to the current moment - if something occurred at a particular timestamp, then it was potentially influenced by everything that happened before it. This idea can be generalized into a causal clock that explicitly tracks causes (dependencies) rather than simply assuming that everything that preceded a timestamp was relevant. Of course, the usual assumption is that we should only worry about the state of the specific system rather than the whole world.
> 时间戳实际上是一种速记值，用来表示从宇宙开始到当前时刻的世界状态--如果某件事发生在某个特定的时间戳，那么它就有可能受到在它之前发生的所有事情的影响。这种想法可以推广到因果时钟中，明确追踪原因（依赖关系），而不是简单地假设在时间戳之前发生的所有事情都是相关的。当然，通常的假设是，我们应该只关心特定系统的状态，而不是整个世界。

Assuming that time progresses at the same rate everywhere - and that is a big assumption which I'll return to in a moment - time and timestamps have several useful interpretations when used in a program. The three interpretations are:
- Order
- Duration
- Interpretation
> 假设时间在任何地方都以相同的速度流逝--这是个很大的假设，我稍后再谈--那么在程序中使用时间和时间戳时，有几种有用的解释。这三种解释是
	顺序
	持续时间
	解释

_Order_. When I say that time is a source of order, what I mean is that:
- we can attach timestamps to unordered events to order them
- we can use timestamps to enforce a specific ordering of operations or the delivery of messages (for example, by delaying an operation if it arrives out of order)
- we can use the value of a timestamp to determine whether something happened chronologically before something else
> 秩序当我说时间是秩序的源泉时，我的意思是：
	我们可以为无序事件附加时间戳，使其有序化
	我们可以使用时间戳来强制执行操作或信息传递的特定顺序（例如，如果操作不按顺序到达，则延迟该操作）。
	我们可以使用时间戳的值来确定某件事情是否按时间顺序发生在其他事情之前

_Interpretation_ - time as a universally comparable value. The absolute value of a timestamp can be interpreted as a date, which is useful for people. Given a timestamp of when a downtime started from a log file, you can tell that it was last Saturday, when there was a [thunderstorm](https://twitter.com/AWSFail/statuses/218915147060752384).
> 解释--时间是一个普遍可比的值。时间戳的绝对值可以解释为日期，这对人们很有用。根据日志文件中停机时间的时间戳，可以知道是上周六，当时有雷雨。

_Duration_ - durations measured in time have some relation to the real world. Algorithms generally don't care about the absolute value of a clock or its interpretation as a date, but they might use durations to make some judgment calls. In particular, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency.
> 持续时间--以时间衡量的持续时间与现实世界有一定关系。算法通常并不关心时钟的绝对值或将其解释为日期，但它们可能会使用持续时间来做出一些判断。特别是，花费在等待上的时间可以提供一些线索，说明系统是被分割了，还是仅仅出现了高延迟。

By their nature, the components of distributed systems do not behave in a predictable manner. They do not guarantee any specific order, rate of advance, or lack of delay. Each node does have some local order - as execution is (roughly) sequential - but these local orders are independent of each other.
> 就其本质而言，分布式系统的各组成部分并不能以可预测的方式运行。它们不能保证任何特定的顺序、前进速度或无延迟。每个节点都有一些本地顺序--因为执行是（大致）按顺序进行的--但这些本地顺序是相互独立的。

Imposing (or assuming) order is one way to reduce the space of possible executions and possible occurrences. Humans have a hard time reasoning about things when things can happen in any order - there just are too many permutations to consider.
> 强加（或假设）秩序是减少可能执行和可能发生的空间的一种方法。当事情可能以任何顺序发生时，人类就很难对事情进行推理--要考虑的排列组合实在是太多了。

## Does time progress at the same rate everywhere?
We all have an intuitive concept of time based on our own experience as individuals. Unfortunately, that intuitive notion of time makes it easier to picture total order rather than partial order. It's easier to picture a sequence in which things happen one after another, rather than concurrently. It is easier to reason about a single order of messages than to reason about messages arriving in different orders and with different delays.
> 我们每个人都根据自己的个人经验对时间有一个直观的概念。不幸的是，这种直观的时间概念让我们更容易想象出整体秩序，而不是局部秩序。我们更容易想象出事情一个接一个发生的顺序，而不是同时发生的顺序。推理单一顺序的信息比推理以不同顺序和不同延迟到达的信息更容易。

However, when implementing distributing systems we want to avoid making strong assumptions about time and order, because the stronger the assumptions, the more fragile a system is to issues with the "time sensor" - or the onboard clock. Furthermore, imposing an order carries a cost. The more temporal nondeterminism that we can tolerate, the more we can take advantage of distributed computation.
> 然而，在实现分布式系统时，我们希望避免对时间和顺序做出强有力的假设，因为假设越强，系统就越容易受到 “时间传感器” 或板载时钟的问题的影响。此外，实施命令是有成本的。我们可以容忍的时间不确定性越多，我们就越能利用分布式计算。

### Time with a "global-clock" assumption
The global clock assumption is that there is a global clock of perfect accuracy, and that everyone has access to that clock. This is the way we tend to think about time, because in human interactions small differences in time don't really matter.
> 全局时钟假设是有一个完全准确的全局时钟，并且每个人都可以访问该时钟。 这是我们倾向于思考时间的方式，因为在人类互动中，时间上的微小差异并不重要。
![[global-clock.png]]

The global clock is basically a source of total order (exact order of every operation on all nodes even if those nodes have never communicated).
> 全局时钟基本上是总顺序的来源（所有节点上的每个操作的确切顺序，即使这些节点从未通信过）。

However, this is an idealized view of the world: in reality, clock synchronization is only possible to a limited degree of accuracy. This is limited by the lack of accuracy of clocks in commodity computers, by latency if a clock synchronization protocol such as [NTP](http://en.wikipedia.org/wiki/Network_Time_Protocol) is used and fundamentally by [the nature of spacetime](http://en.wikipedia.org/wiki/Time_dilation).
> 然而，这是一个理想化的世界观：在现实中，时钟同步只能在有限的精度范围内实现。 这受到商品计算机中时钟缺乏准确性的限制，如果使用诸如ntp之类的时钟同步协议，则会受到延迟的限制，并且从根本上受到时空的性质的限制。

Assuming that clocks on distributed nodes are perfectly synchronized means assuming that clocks start at the same value and never drift apart. It's a nice assumption because you can use timestamps freely to determine a global total order - bound by clock drift rather than latency - but this is a nontrivial operational challenge and a potential source of anomalies. There are many different scenarios where a simple failure - such as a user accidentally changing the local time on a machine, or an out-of-date machine joining a cluster, or synchronized clocks drifting at slightly different rates and so on that can cause hard-to-trace anomalies.
> 假设分布式节点上的时钟完全同步意味着假设时钟以相同的值开始并且从不漂移。 这是一个很好的假设，因为您可以自由使用时间戳来确定全局总顺序-受时钟漂移而不是延迟约束-但这是一个非平凡的操作挑战，也是异常的潜在来源。 在许多不同的情况下，一个简单的故障--例如用户意外地更改了计算机上的本地时间，或者一台过时的计算机加入了集群，或者同步时钟以稍微不同的速率漂移，等等，可能导致难以跟踪的异常。

Nevertheless, there are some real-world systems that make this assumption. Facebook's Cassandra is an example of a system that assumes clocks are synchronized. It uses timestamps to resolve conflicts between writes - the write with the newer timestamp wins. This means that if clocks drift, new data may be ignored or overwritten by old data; again, this is an operational challenge (and from what I've heard, one that people are acutely aware of). Another interesting example is Google's Spanner: the paper describes their TrueTime API, which synchronizes time but also estimates worst-case clock drift.
> 尽管如此，还是有一些现实世界的系统做出了这种假设。 Facebook的Cassandra是假设时钟同步的系统的一个例子。 它使用时间戳来解决写入之间的冲突-具有较新时间戳的写入获胜。 这意味着，如果时钟漂移，新数据可能会被忽略或被旧数据复盖;再次，这是一个操作挑战（从我所听到的，人们敏锐地意识到的）。 另一个有趣的例子是谷歌的Spanner：该论文描述了他们的TrueTime API，它同步时间，但也估计了最坏情况的时钟漂移。

### Time with a "Local-clock" assumption
The second, and perhaps more plausible assumption is that each machine has its own clock, but there is no global clock. It means that you cannot use the local clock in order to determine whether a remote timestamp occurred before or after a local timestamp; in other words, you cannot meaningfully compare timestamps from two different machines.
> 第二种假设可能更合理，即每台机器都有自己的时钟，但没有全局时钟。这意味着无法使用本地时钟来确定远程时间戳是发生在本地时间戳之前还是之后；换句话说，无法对来自两台不同机器的时间戳进行有意义的比较。
![[local-clock.png]]

The local clock assumption corresponds more closely to the real world. It assigns a partial order: events on each system are ordered but events cannot be ordered across systems by only using a clock.
> 本地时钟假设与现实世界更接近。 它分配一个部分顺序：每个系统上的事件都是有序的，但仅使用时钟不能跨系统对事件进行排序。

However, you can use timestamps to order events on a single machine; and you can use timeouts on a single machine as long as you are careful not to allow the clock to jump around. Of course, on a machine controlled by an end-user this is probably assuming too much: for example, a user might accidentally change their date to a different value while looking up a date using the operating system's date control.
> 不过，您可以在单台机器上使用时间戳对事件进行排序；您也可以在单台机器上使用超时，只要注意不让时钟跳动即可。当然，在由终端用户控制的机器上，这样做的假设可能过高：例如，用户在使用操作系统的日期控件查找日期时，可能会不小心将日期改成不同的值。

### Time with a "No-clock" assumption
Finally, there is the notion of logical time. Here, we don't use clocks at all and instead track causality in some other way. Remember, a timestamp is simply a shorthand for the state of the world up to that point - so we can use counters and communication to determine whether something happened before, after or concurrently with something else.
> 最后，还有逻辑时间的概念。 在这里，我们根本不使用时钟，而是以其他方式跟踪因果关系。 请记住，时间戳只是世界状态的简写，因此我们可以使用计数器和通信来确定事情是在发生之前，之后还是与其他事情同时发生。

This way, we can determine the order of events between different machines, but cannot say anything about intervals and cannot use timeouts (since we assume that there is no "time sensor"). This is a partial order: events can be ordered on a single system using a counter and no communication, but ordering events across systems requires a message exchange.
> 通过这种方式，我们可以确定不同机器之间的事件顺序，但无法确定时间间隔，也无法使用超时（因为我们假设不存在 "时间传感器"）。这只是部分排序：在单个系统上，可以使用计数器排序事件，无需通信，但在不同系统间排序事件则需要交换信息。

## How is time used in a distributed system?
What is the benefit of time?
1. Time can define order across a system (without communication)
2. Time can define boundary conditions for algorithms
> 时间的好处是什么？
	时间可以定义整个系统的秩序（无需通信）
	时间可以定义算法的边界条件

The order of events is important in distributed systems, because many properties of distributed systems are defined in terms of the order of operations/events:
- where correctness depends on (agreement on) correct event ordering, for example serializability in a distributed database
- order can be used as a tie breaker when resource contention occurs, for example if there are two orders for a widget, fulfill the first and cancel the second one
> 事件的顺序在分布式系统中非常重要，因为分布式系统的许多属性都是根据操作/事件的顺序定义的：
	正确性取决于（对）正确的事件排序（达成一致），例如分布式数据库中的可序列化性
	当发生资源争夺时，顺序可用作打破平局的因素，例如，如果一个小部件有两个订单，则执行第一个订单并取消第二个订单

A global clock would allow operations on two different machines to be ordered without the two machines communicating directly. Without a global clock, we need to communicate in order to determine order.
> 全局时钟可以让两台不同机器上的操作有序进行，而无需两台机器直接通信。如果没有全局时钟，我们就需要通过通信来确定顺序。

Time can also be used to define boundary conditions for algorithms - specifically, to distinguish between "high latency" and "server or network link is down". This is a very important use case; in most real-world systems timeouts are used to determine whether a remote machine has failed, or whether it is simply experiencing high network latency. Algorithms that make this determination are called failure detectors.
> 时间还可用于定义算法的边界条件 - 具体来说，用于区分“高延迟”和“服务器或网络链接已关闭”。这是一个非常重要的用例;在大多数实际系统中，超时用于确定远程计算机是否出现故障，或者它是否只是遇到了高网络延迟。做出此决定的算法称为故障检测器。

## Vector clocks (time for causal order)
Lamport clocks and vector clocks are replacements for physical clocks which rely on counters and communication to determine the order of events across a distributed system. These clocks provide a counter that is comparable across different nodes.
> Lamport 时钟和矢量时钟是物理时钟的替代品，它们依靠计数器和通信来确定分布式系统中的事件顺序。这些时钟提供的计数器在不同节点之间具有可比性。

_A Lamport clock_ is simple. Each process maintains a counter using the following rules:
- Whenever a process does work, increment the counter
- Whenever a process sends a message, include the counter
- When a message is received, set the counter to `max(local_counter, received_counter) + 1`
> Lamport 时钟很简单。每个进程使用以下规则维护一个计数器：
	每当进程正常工作时，递增计数器
	每当进程发送消息时，请包含计数器
	收到消息时，将计数器设置为 max（local_counter， received_counter） + 1

Expressed as code:
```
function LamportClock() {
  this.value = 1;
}

LamportClock.prototype.get = function() {
  return this.value;
}

LamportClock.prototype.increment = function() {
  this.value++;
}

LamportClock.prototype.merge = function(other) {
  this.value = Math.max(this.value, other.value) + 1;
}
```

A [Lamport clock](http://en.wikipedia.org/wiki/Lamport_timestamps) allows counters to be compared across systems, with a caveat: Lamport clocks define a partial order. If `timestamp(a) < timestamp(b)`:
- `a` may have happened before `b` or
- `a` may be incomparable with `b`
> Lamport 时钟允许在不同系统间比较计数器，但有一个注意事项：Lamport 时钟定义了部分顺序。如果 timestamp(a) < timestamp(b)：
	a 可能发生在 b 之前，或者
	a 可能无法与 b 进行比较

This is known as clock consistency condition: if one event comes before another, then that event's logical clock comes before the others. If `a` and `b` are from the same causal history, e.g. either both timestamp values were produced on the same process; or `b` is a response to the message sent in `a` then we know that `a` happened before `b`.
> 这称为时钟一致性条件：如果一个事件先于另一个事件，则该事件的逻辑时钟先于其他事件。 如果a和b来自相同的因果历史，例如两个时间戳值都是在同一个进程上产生的;或者b是对a中发送的消息的响应，那么我们知道a发生在b之前。

Intuitively, this is because a Lamport clock can only carry information about one timeline / history; hence, comparing Lamport timestamps from systems that never communicate with each other may cause concurrent events to appear to be ordered when they are not.
> 直观地说，这是因为一个 Lamport 时钟只能携带一个时间线/历史的信息；因此，如果比较从未相互通信的系统的 Lamport 时间戳，可能会导致同时发生的事件看起来是有序的，而实际上并非如此。

Imagine a system that after an initial period divides into two independent subsystems which never communicate with each other.
> 试想，一个系统在初始阶段后会分成两个独立的子系统，而这两个子系统永远不会相互通信。

For all events in each independent system, if a happened before b, then ts(a) < ts(b); but if you take two events from the different independent systems (e.g. events that are not causally related) then you cannot say anything meaningful about their relative order. While each part of the system has assigned timestamps to events, those timestamps have no relation to each other. Two events may appear to be ordered even though they are unrelated.
> 对于每个独立系统中的所有事件，如果 a 发生在 b 之前，那么 ts(a) < ts(b)；但是，如果你从不同的独立系统中选取两个事件（例如，没有因果关系的事件），那么你就无法对它们的相对顺序做出任何有意义的说明。虽然系统的每个部分都为事件分配了时间戳，但这些时间戳之间没有任何关系。两个事件可能看起来是有序的，即使它们之间并无关联。

However - and this is still a useful property - from the perspective of a single machine, any message sent with ts(a) will receive a response with ts(b) which is > ts(a).
> 然而--这仍然是一个有用的特性--从单台机器的角度来看，任何以 ts(a) 发送的报文都会收到一个以 ts(b) 发送的响应，这个响应大于 ts(a)。

_A vector clock_ is an extension of Lamport clock, which maintains an array `[ t1, t2, ... ]` of N logical clocks - one per each node. Rather than incrementing a common counter, each node increments its own logical clock in the vector by one on each internal event. Hence the update rules are:
- Whenever a process does work, increment the logical clock value of the node in the vector
- Whenever a process sends a message, include the full vector of logical clocks
- When a message is received:
    - update each element in the vector to be `max(local, received)`
    - increment the logical clock value representing the current node in the vector
> 矢量时钟是 Lamport 时钟的扩展，它维护一个由 N 个逻辑时钟组成的数组 [ t1、t2、......] - 每个节点一个。每个节点在每次内部事件发生时，都会将自己在矢量中的逻辑时钟递增一个，而不是递增一个公共计数器。因此，更新规则如下
	每当进程执行工作时，就递增向量中节点的逻辑时钟值
	每当有进程发送信息时，就包含逻辑时钟的整个向量
	收到信息时
		将矢量中的每个元素更新为 max（local，received）
		递增矢量中代表当前节点的逻辑时钟值

Again, expressed as code:
```
function VectorClock(value) {
  // expressed as a hash keyed by node id: e.g. { node1: 1, node2: 3 }
  this.value = value || {};
}

VectorClock.prototype.get = function() {
  return this.value;
};

VectorClock.prototype.increment = function(nodeId) {
  if(typeof this.value[nodeId] == 'undefined') {
    this.value[nodeId] = 1;
  } else {
    this.value[nodeId]++;
  }
};

VectorClock.prototype.merge = function(other) {
  var result = {}, last,
      a = this.value,
      b = other.value;
  // This filters out duplicate keys in the hash
  (Object.keys(a)
    .concat(b))
    .sort()
    .filter(function(key) {
      var isDuplicate = (key == last);
      last = key;
      return !isDuplicate;
    }).forEach(function(key) {
      result[key] = Math.max(a[key] || 0, b[key] || 0);
    });
  this.value = result;
};
```

This illustration ([source](http://en.wikipedia.org/wiki/Vector_clock)) shows a vector clock:
![[vector_clock.svg.png]]
Each of the three nodes (A, B, C) keeps track of the vector clock. As events occur, they are timestamped with the current value of the vector clock. Examining a vector clock such as `{ A: 2, B: 4, C: 1 }` lets us accurately identify the messages that (potentially) influenced that event.
> 三个节点（A、B、C）分别跟踪矢量时钟。当事件发生时，它们会以矢量时钟的当前值作为时间戳。通过查看矢量时钟，如 { A: 2, B: 4, C: 1 }，我们可以准确识别（可能）影响该事件的信息。

The issue with vector clocks is mainly that they require one entry per node, which means that they can potentially become very large for large systems. A variety of techniques have been applied to reduce the size of vector clocks (either by performing periodic garbage collection, or by reducing accuracy by limiting the size).
> 矢量时钟的问题主要在于每个节点需要一个条目，这意味着对于大型系统来说，矢量时钟可能会变得非常庞大。为了减小矢量时钟的体积，人们已经采用了多种技术（通过执行周期性垃圾回收，或通过限制体积来降低精度）。

## Failure detectors (time for cutoff)
As I stated earlier, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency. In this case, we don't need to assume a global clock of perfect accuracy - it is simply enough that there is a reliable-enough local clock.
> 如前所述，等待时间的长短可以提供一些线索，说明系统是被分割了，还是仅仅出现了高延迟。在这种情况下，我们不需要假定全局时钟具有完美的准确性，只要有一个足够可靠的本地时钟就足够了。

Given a program running on one node, how can it tell that a remote node has failed? In the absence of accurate information, we can infer that an unresponsive remote node has failed after some reasonable amount of time has passed.
> 在一个节点上运行的程序如何判断远程节点已经失效？在缺乏准确信息的情况下，我们可以推断，在经过一定的合理时间后，无响应的远程节点已经失效。

But what is a "reasonable amount"? This depends on the latency between the local and remote nodes. Rather than explicitly specifying algorithms with specific values (which would inevitably be wrong in some cases), it would be nicer to deal with a suitable abstraction.
> 但什么是 "合理数量"？这取决于本地节点和远程节点之间的延迟。与其明确指定具有特定值的算法（这在某些情况下难免会出错），还不如使用一个合适的抽象概念。

A failure detector is a way to abstract away the exact timing assumptions. Failure detectors are implemented using heartbeat messages and timers. Processes exchange heartbeat messages. If a message response is not received before the timeout occurs, then the process suspects the other process.
> 故障检测器是一种抽象出精确定时假设的方法。 故障检测器使用检测信号消息和计时器来实现。 进程交换心跳消息。 如果在超时发生之前未收到消息响应，则该进程怀疑另一进程。

A failure detector based on a timeout will carry the risk of being either overly aggressive (declaring a node to have failed) or being overly conservative (taking a long time to detect a crash). How accurate do failure detectors need to be for them to be usable?
> 基于超时的故障检测器有可能过于激进（宣布节点故障）或过于保守（需要很长时间才能检测到崩溃）。故障检测器需要多精确才能使用？

[Chandra et al.](http://www.google.com/search?q=Unreliable%20Failure%20Detectors%20for%20Reliable%20Distributed%20Systems) (1996) discuss failure detectors in the context of solving consensus - a problem that is particularly relevant since it underlies most replication problems where the replicas need to agree in environments with latency and network partitions.
> Chandra et al. （1996） 在解决共识的背景下讨论了故障检测器 - 这个问题特别相关，因为它是大多数复制问题的基础，其中副本需要在具有延迟和网络分区的环境中保持一致。

They characterize failure detectors using two properties, completeness and accuracy:
	Strong completeness.
		Every crashed process is eventually suspected by every correct process.
	Weak completeness.
		Every crashed process is eventually suspected by some correct process.
	Strong accuracy.
		No correct process is suspected ever.
	Weak accuracy.
		Some correct process is never suspected.
> 他们利用完整性和准确性这两个特性来描述故障检测器：
	强完整性。
		每个崩溃的进程最终都会被每个正确的进程所怀疑。
	弱完整性。
		每个崩溃的进程最终都会被某些正确的进程怀疑。
	强准确性。
		从未怀疑过任何正确进程。
	弱准确性。
		从未怀疑过某个正确进程。

Completeness is easier to achieve than accuracy; indeed, all failure detectors of importance achieve it - all you need to do is not to wait forever to suspect someone. Chandra et al. note that a failure detector with weak completeness can be transformed to one with strong completeness (by broadcasting information about suspected processes), allowing us to concentrate on the spectrum of accuracy properties.
> 完备性比准确性更容易实现；事实上，所有重要的故障检测器都能实现这一点--你所需要做的就是不要永远等待怀疑对象。钱德拉等人指出，具有弱完备性的故障检测器可以转变为具有强完备性的故障检测器（通过广播可疑进程的信息），从而让我们专注于准确性特性的频谱。

Avoiding incorrectly suspecting non-faulty processes is hard unless you are able to assume that there is a hard maximum on the message delay. That assumption can be made in a synchronous system model - and hence failure detectors can be strongly accurate in such a system. Under system models that do not impose hard bounds on message delay, failure detection can at best be eventually accurate.
> 避免错误地怀疑非错误进程是很困难的，除非您能够假设消息延迟有一个硬的最大值。 这一假设可以在同步系统模型中进行-因此在这样的系统中，故障检测器可以非常精确。 在不对消息延迟施加硬边界的系统模型下，故障检测最多可以最终准确。

Chandra et al. show that even a very weak failure detector - the eventually weak failure detector ⋄W (eventually weak accuracy + weak completeness) - can be used to solve the consensus problem. The diagram below (from the paper) illustrates the relationship between system models and problem solvability:
> Chandra 等人表明，即使是非常弱的故障检测器 - 最终弱的故障检测器 ⋄W （最终的弱精度 + 弱完整性） - 也可以用于解决共识问题。下图（来自论文）说明了系统模型和问题解决能力之间的关系：
![[chandra_failure_detectors.png]]

As you can see above, certain problems are not solvable without a failure detector in asynchronous systems. This is because without a failure detector (or strong assumptions about time bounds e.g. the synchronous system model), it is not possible to tell whether a remote node has crashed, or is simply experiencing high latency. That distinction is important for any system that aims for single-copy consistency: failed nodes can be ignored because they cannot cause divergence, but partitioned nodes cannot be safely ignored.
> 如上所述，在异步系统中，如果没有故障检测器，某些问题是无法解决的。这是因为，如果没有故障检测器（或关于时间界限的强假设，如同步系统模型），就无法判断远程节点是崩溃了，还是仅仅遇到了高延迟。这种区别对于任何追求单拷贝一致性的系统来说都很重要：故障节点可以忽略，因为它们不会导致分歧，但分区节点却不能被安全地忽略。

How can one implement a failure detector? Conceptually, there isn't much to a simple failure detector, which simply detects failure when a timeout expires. The most interesting part relates to how the judgments are made about whether a remote node has failed.
> 如何实现故障检测器？从概念上讲，简单的故障检测器并没有什么特别之处，它只是在超时时检测故障。最有趣的部分在于如何判断远程节点是否发生故障。

Ideally, we'd prefer the failure detector to be able to adjust to changing network conditions and to avoid hardcoding timeout values into it. For example, Cassandra uses an accrual failure detector, which is a failure detector that outputs a suspicion level (a value between 0 and 1) rather than a binary "up" or "down" judgment. This allows the application using the failure detector to make its own decisions about the tradeoff between accurate detection and early detection.
> 理想情况下，我们希望故障检测器能够根据不断变化的网络条件进行调整，并避免硬编码超时值。例如，Cassandra 使用应计故障检测器，这种故障检测器输出的是怀疑级别（介于 0 和 1 之间的值），而不是二进制的 "上 "或 "下 "判断。这允许使用故障检测器的应用程序自行决定在准确检测和早期检测之间进行权衡。

## Time, order and performance
If you're writing a distributed system, you presumably own more than one computer. The natural (and realistic) view of the world is a partial order, not a total order. You can transform a partial order into a total order, but this requires communication, waiting and imposes restrictions that limit how many computers can do work at any particular point in time.
> 如果你正在编写一个分布式系统，那么你可能拥有不止一台计算机。自然的（也是现实的）世界观是部分秩序，而不是全部秩序。你可以将部分顺序转化为整体顺序，但这需要通信、等待，并施加了限制，限制了在任何特定时间点有多少台计算机可以工作。

All clocks are mere approximations bound by either network latency (logical time) or by physics. Even keeping a simple integer counter in sync across multiple nodes is a challenge.
> 所有时钟都只是近似值，受到网络延迟（逻辑时间）或物理学的限制。即使是一个简单的整数计数器，在多个节点之间保持同步也是一项挑战。

While time and order are often discussed together, time itself is not such a useful property. Algorithms don't really care about time as much as they care about more abstract properties:
- the causal ordering of events
- failure detection (e.g. approximations of upper bounds on message delivery)
- consistent snapshots (e.g. the ability to examine the state of a system at some point in time; not discussed here)
> 虽然时间和顺序经常被放在一起讨论，但时间本身并不是一个非常有用的属性。算法其实并不关心时间，它们关心的是更抽象的属性：
	事件的因果顺序
	故障检测（如信息传递上限的近似值）
	一致快照（例如，在某个时间点检查系统状态的能力；此处不作讨论）

Imposing a total order is possible, but expensive. It requires you to proceed at the common (lowest) speed. Often the easiest way to ensure that events are delivered in some defined order is to nominate a single (bottleneck) node through which all operations are passed.
> 强制执行全面命令是可能的，但代价高昂。它要求你以普通（最低）速度进行操作。通常情况下，确保事件以某种确定的顺序传送的最简单方法是指定一个（瓶颈）节点，所有操作都通过该节点进行。

Is time / order / synchronicity really necessary? It depends. In some use cases, we want each intermediate operation to move the system from one consistent state to another. For example, in many cases we want the responses from a database to represent all of the available information, and we want to avoid dealing with the issues that might occur if the system could return an inconsistent result.
> 时间/秩序/同步性真的有必要吗？这要看情况。在某些用例中，我们希望每个中间操作都能将系统从一个一致的状态转移到另一个一致的状态。例如，在许多情况下，我们希望数据库的响应能代表所有可用信息，我们希望避免处理系统返回不一致结果时可能出现的问题。

But in other cases, we might not need that much time / order / synchronization. For example, if you are running a long running computation, and don't really care about what the system does until the very end - then you don't really need much synchronization as long as you can guarantee that the answer is correct.
> 但在其他情况下，我们可能并不需要那么多时间/顺序/同步。例如，如果你正在运行一个长时间运行的计算，并且直到最后才真正关心系统做了什么，那么只要你能保证答案是正确的，你就不需要太多的同步。

Synchronization is often applied as a blunt tool across all operations, when only a subset of cases actually matter for the final outcome. When is order needed to guarantee correctness? The CALM theorem - which I will discuss in the last chapter - provides one answer.
> 同步通常作为一个钝工具应用于所有操作，当只有一小部分情况实际上对最终结果很重要时。 何时需要订单以保证正确性？ 平静定理-我将在最后一章中讨论-提供了一个答案。

In other cases, it is acceptable to give an answer that only represents the best known estimate - that is, is based on only a subset of the total information contained in the system. In particular, during a network partition one may need to answer queries with only a part of the system being accessible. In other use cases, the end user cannot really distinguish between a relatively recent answer that can be obtained cheaply and one that is guaranteed to be correct and is expensive to calculate. For example, is the Twitter follower count for some user X, or X+1? Or are movies A, B and C the absolutely best answers for some query? Doing a cheaper, mostly correct "best effort" can be acceptable.
> 在其他情况下，给出仅代表最已知估计的答案是可以接受的，即仅基于系统中包含的总信息的子集。特别是，在网络分区期间，人们可能需要仅在系统的一部分可访问的情况下回答查询。在其他用例中，最终用户无法真正区分可以廉价获得的相对较新的答案和保证正确且计算成本昂贵的答案。例如，某个用户的 Twitter 关注者数量是 X 还是 X+1？或者电影 A、B 和 C 是否绝对是某些查询的最佳答案？做一个更便宜、大部分正确的“尽力而为”是可以接受的。

# 4. Replication
The replication problem is one of many problems in distributed systems. I've chosen to focus on it over other problems such as leader election, failure detection, mutual exclusion, consensus and global snapshots because it is often the part that people are most interested in. One way in which parallel databases are differentiated is in terms of their replication features, for example. Furthermore, replication provides a context for many subproblems, such as leader election, failure detection, consensus and atomic broadcast.
> 复制问题是分布式系统中的众多问题之一。我选择将重点放在复制问题上，而不是其他问题上，如领导者选举、故障检测、互斥、共识和全局快照，因为这往往是人们最感兴趣的部分。例如，并行数据库的差异化之一就在于其复制功能。此外，复制还为领导者选举、故障检测、共识和原子广播等许多子问题提供了背景。

Replication is a group communication problem. What arrangement and communication pattern gives us the performance and availability characteristics we desire? How can we ensure fault tolerance, durability and non-divergence in the face of network partitions and simultaneous node failure?
> 复制是一个群体通信问题。怎样的安排和通信模式才能提供我们所期望的性能和可用性特征？面对网络分区和同时发生的节点故障，我们如何确保容错性、持久性和无发散性？

Let's first define what replication looks like. We assume that we have some initial database, and that clients make requests which change the state of the database.
> 让我们先来定义一下复制是什么样子的。我们假设有一个初始数据库，而客户发出的请求会改变数据库的状态。

![[replication-both.png]]
The arrangement and communication pattern can then be divided into several stages:
1. (Request) The client sends a request to a server
2. (Sync) The synchronous portion of the replication takes place
3. (Response) A response is returned to the client
4. (Async) The asynchronous portion of the replication takes place
> 然后，安排和通信模式可以分为几个阶段：
	（请求）客户端向服务器发送请求
	（同步）复制的同步部分发生
	（响应）将响应返回给客户端
	（异步）复制的异步部分发生

This model is loosely based on this article. Note that the pattern of messages exchanged in each portion of the task depends on the specific algorithm: I am intentionally trying to get by without discussing the specific algorithm.
> 该模型大致基于这篇文章。请注意，任务各部分的信息交换模式取决于具体算法：我有意不讨论具体算法。

Given these stages, what kind of communication patterns can we create? And what are the performance and availability implications of the patterns we choose?
> 在这些阶段，我们可以创建什么样的通信模式？我们选择的模式对性能和可用性有什么影响？

## Synchronous replication
The first pattern is synchronous replication (also known as active, or eager, or push, or pessimistic replication). Let's draw what that looks like:
> 第一种模式是同步复制（也称为主动复制、急切复制、推送复制或悲观复制）。让我们画出它的样子：
![[replication-sync.png]]

Here, we can see three distinct stages: first, the client sends the request. Next, what we called the synchronous portion of replication takes place. The term refers to the fact that the client is blocked - waiting for a reply from the system.
> 在这里，我们可以看到三个不同的阶段：首先，客户端发送请求。接下来是我们所说的复制的同步部分。这个术语指的是客户端被阻塞--等待系统的回复。

During the synchronous phase, the first server contacts the two other servers and waits until it has received replies from all the other servers. Finally, it sends a response to the client informing it of the result (e.g. success or failure).
> 在同步阶段，第一台服务器会联系另外两台服务器，并等待收到所有其他服务器的回复。最后，它向客户机发送回复，告知结果（如成功或失败）。

All this seems straightforward. What can we say of this specific arrangement of communication patterns, without discussing the details of the algorithm during the synchronous phase? First, observe that this is a write N - of - N approach: before a response is returned, it has to be seen and acknowledged by every server in the system.
> 所有这一切似乎都是直截了当的。在不讨论同步阶段算法细节的情况下，我们能对这种通信模式的具体安排说些什么呢？首先，请注意这是一种写 N - of - N 的方法：在返回响应之前，系统中的每台服务器都必须看到并确认该响应。

From a performance perspective, this means that the system will be as fast as the slowest server in it. The system will also be very sensitive to changes in network latency, since it requires every server to reply before proceeding.
> 从性能角度看，这意味着系统的速度将与其中最慢的服务器一样快。该系统对网络延迟的变化也非常敏感，因为它要求每台服务器都先回复，然后才能继续。

Given the N-of-N approach, the system cannot tolerate the loss of any servers. When a server is lost, the system can no longer write to all the nodes, and so it cannot proceed. It might be able to provide read-only access to the data, but modifications are not allowed after a node has failed in this design.
> 考虑到 N-of-N 方法，系统不能容忍任何服务器的丢失。当服务器丢失时，系统就无法再向所有节点写入数据，因此无法继续运行。系统也许可以提供对数据的只读访问，但在这种设计中，节点失效后是不允许修改的。

This arrangement can provide very strong durability guarantees: the client can be certain that all N servers have received, stored and acknowledged the request when the response is returned. In order to lose an accepted update, all N copies would need to be lost, which is about as good a guarantee as you can make.
> 这种安排可以提供非常强的耐久性保证：客户端可以确定，当响应返回时，所有 N 台服务器都已接收、存储并确认了请求。要想丢失已接受的更新，就必须丢失所有 N 份副本，这已经是最好的保证了。

## Asynchronous replication
Let's contrast this with the second pattern - asynchronous replication (a.k.a. passive replication, or pull replication, or lazy replication). As you may have guessed, this is the opposite of synchronous replication:
> 让我们将其与第二种模式进行对比 - 异步复制（也称为被动复制、拉取复制或延迟复制）。您可能已经猜到了，这与同步复制相反：
![[replication-async.png]]

Here, the master (/leader / coordinator) immediately sends back a response to the client. It might at best store the update locally, but it will not do any significant work synchronously and the client is not forced to wait for more rounds of communication to occur between the servers.
> 在这种情况下，主服务器（/领导者/协调者）会立即向客户端发回响应。它最多只能将更新存储在本地，但不会同步进行任何重要工作，客户端也不会被迫等待服务器之间发生更多轮通信。

At some later stage, the asynchronous portion of the replication task takes place. Here, the master contacts the other servers using some communication pattern, and the other servers update their copies of the data. The specifics depend on the algorithm in use.
> 在稍后阶段，复制任务的异步部分开始执行。在此，主服务器使用某种通信模式与其他服务器联系，其他服务器更新它们的数据副本。具体细节取决于所使用的算法。

What can we say of this specific arrangement without getting into the details of the algorithm? Well, this is a write 1 - of - N approach: a response is returned immediately and update propagation occurs sometime later.
> 如果不深入了解算法的细节，我们能对这种特定的安排说些什么呢？嗯，这是一种写入 1 - of - N 的方法：立即返回响应，并在稍后的某个时间进行更新传播。

From a performance perspective, this means that the system is fast: the client does not need to spend any additional time waiting for the internals of the system to do their work. The system is also more tolerant of network latency, since fluctuations in internal latency do not cause additional waiting on the client side.
> 从性能角度看，这意味着系统运行速度快：客户端无需花费额外时间等待系统内部完成工作。系统对网络延迟的容忍度也更高，因为内部延迟的波动不会给客户端带来额外的等待时间。

This arrangement can only provide weak, or probabilistic durability guarantees. If nothing goes wrong, the data is eventually replicated to all N machines. However, if the only server containing the data is lost before this can take place, the data is permanently lost.
> 这种安排只能提供较弱的或概率性的耐用性保证。如果不出意外，数据最终会复制到所有 N 台机器上。但是，如果包含数据的唯一服务器在复制之前丢失，数据就会永久丢失。

Given the 1-of-N approach, the system can remain available as long as at least one node is up (at least in theory, though in practice the load will probably be too high). A purely lazy approach like this provides no durability or consistency guarantees; you may be allowed to write to the system, but there are no guarantees that you can read back what you wrote if any faults occur.
> 鉴于 1-of-N 方法，只要至少有一个节点启动，系统就可以保持可用（至少在理论上是这样，尽管在实践中负载可能太高）。像这样的纯惰性方法不提供持久性或一致性保证;您可以写入系统，但不能保证在发生任何错误时可以读回您写入的内容。

Finally, it's worth noting that passive replication cannot ensure that all nodes in the system always contain the same state. If you accept writes at multiple locations and do not require that those nodes synchronously agree, then you will run the risk of divergence: reads may return different results from different locations (particularly after nodes fail and recover), and global constraints (which require communicating with everyone) cannot be enforced.
> 最后，值得注意的是，被动复制无法确保系统中的所有节点始终包含相同的状态。如果你接受多个位置的写入，却不要求这些节点同步同意，那么你将面临分歧的风险：从不同位置读取可能会返回不同的结果（尤其是在节点故障和恢复之后），而且全局约束（需要与每个人通信）也无法执行。

## An overview of major replication approaches
There are many, many different ways to categorize replication techniques. The second distinction (after sync vs. async) I'd like to introduce is between:
- Replication methods that prevent divergence (single copy systems) and
- Replication methods that risk divergence (multi-master systems)
> 复制技术有很多很多不同的分类方法。继同步与异步之后，我想介绍的第二个区别是同步与异步之间的区别：
	防止分歧的复制方法（单拷贝系统）和
	可能出现分歧的复制方法（多主系统）

The first group of methods has the property that they "behave like a single system". In particular, when partial failures occur, the system ensures that only a single copy of the system is active. Furthermore, the system ensures that the replicas are always in agreement. This is known as the consensus problem.
> 第一组方法具有 "表现得像单一系统 "的特性。特别是，当出现部分故障时，系统会确保只有一个系统副本处于活动状态。此外，系统还能确保副本始终保持一致。这就是所谓的共识问题。

Several processes (or computers) achieve consensus if they all agree on some value. More formally:
1. Agreement: Every correct process must agree on the same value.
2. Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.
3. Termination: All processes eventually reach a decision.
4. Validity: If all correct processes propose the same value V, then all correct processes decide V.
> 如果多个进程（或计算机）都同意某些值，则它们会达成共识。更正式地说：
	一致：每个正确的过程都必须就相同的值达成一致。
	完整性：每个正确的过程最多决定一个值，如果它决定某个值，那么它一定是由某个过程提出的。
	终止：所有进程最终都会做出决定。
	有效性：如果所有正确的过程提出相同的值V，那么所有正确的过程都会决定V。

Mutual exclusion, leader election, multicast and atomic broadcast are all instances of the more general problem of consensus. Replicated systems that maintain single copy consistency need to solve the consensus problem in some way.
> 相互排斥、领导者选举、多播和原子广播都是更普遍的共识问题的实例。保持单一副本一致性的复制系统需要以某种方式解决共识问题。

The replication algorithms that maintain single-copy consistency include:
- 1n messages (asynchronous primary/backup)
- 2n messages (synchronous primary/backup)
- 4n messages (2-phase commit, Multi-Paxos)
- 6n messages (3-phase commit, Paxos with repeated leader election)
> 保持单副本一致性的复制算法包括:
	1n消息（异步主/备份）
	2n消息（同步主/备份）
	4n消息（2阶段提交，多Paxos）
	6n消息（3阶段提交，Paxos与重复领导人选举）

These algorithms vary in their fault tolerance (e.g. the types of faults they can tolerate). I've classified these simply by the number of messages exchanged during an execution of the algorithm, because I think it is interesting to try to find an answer to the question "what are we buying with the added message exchanges?"
> 这些算法的容错能力（例如可容忍的故障类型）各不相同。我将这些算法简单地按照算法执行过程中交换信息的数量进行了分类，因为我认为，试图找到 "我们用增加的信息交换量买到了什么？"这个问题的答案是很有趣的。

The diagram below, adapted from Ryan Barret at [Google](http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html), describes some of the aspects of the different options:
![[google-transact09.png]]

The consistency, latency, throughput, data loss and failover characteristics in the diagram above can really be traced back to the two different replication methods: synchronous replication (e.g. waiting before responding) and asynchronous replication. When you wait, you get worse performance but stronger guarantees. The throughput difference between 2PC and quorum systems will become apparent when we discuss partition (and latency) tolerance.
> 上图中的一致性、延迟、吞吐量、数据丢失和故障转移特性实际上可以追溯到两种不同的复制方法：同步复制（例如，在响应前等待）和异步复制。等待时，性能较差，但保证更强。当我们讨论分区（和延迟）容错时，2PC 和法定人数系统之间的吞吐量差异就会显现出来。

In that diagram, algorithms enforcing weak (/eventual) consistency are lumped up into one category ("gossip"). However, I will discuss replication methods for weak consistency - gossip and (partial) quorum systems - in more detail. The "transactions" row really refers more to global predicate evaluation, which is not supported in systems with weak consistency (though local predicate evaluation can be supported).
> 在该图中，执行弱（/最终）一致性的算法被归为一类（"gossip"）。不过，我将更详细地讨论弱一致性的复制方法--流言和（部分）法定人数系统。事务 "一行实际上指的是全局谓词评估，弱一致性系统不支持全局谓词评估（尽管可以支持局部谓词评估）。

It is worth noting that systems enforcing weak consistency requirements have fewer generic algorithms, and more techniques that can be selectively applied. Since systems that do not enforce single-copy consistency are free to act like distributed systems consisting of multiple nodes, there are fewer obvious objectives to fix and the focus is more on giving people a way to reason about the characteristics of the system that they have.
> 值得注意的是，执行弱一致性要求的系统采用的通用算法较少，而可选择性应用的技术较多。由于不强制执行单拷贝一致性的系统可以像由多个节点组成的分布式系统一样自由运行，因此需要解决的明显目标较少，重点更多在于为人们提供一种推理系统特性的方法。

For example:
 - Client-centric consistency models attempt to provide more intelligible consistency guarantees while allowing for divergence.
 - CRDTs (convergent and commutative replicated datatypes) exploit semilattice properties (associativity, commutativity, idempotency) of certain state and operation-based data types.
 - Confluence analysis (as in the Bloom language) uses information regarding the monotonicity of computations to maximally exploit disorder.
 - PBS (probabilistically bounded staleness) uses simulation and information collected from a real world system to characterize the expected behavior of partial quorum systems.
> 例如
	以客户为中心的一致性模型试图提供更易懂的一致性保证，同时允许分歧。
	CRDT（收敛和换向复制数据类型）利用了某些基于状态和操作的数据类型的半格属性（关联性、换向性、幂等性）。
	汇流分析（如布隆语言）利用计算的单调性信息，最大限度地利用无序性。
	PBS（概率有界僵化）利用模拟和从现实系统中收集的信息来描述部分法定人数系统的预期行为。

## Primary/backup replication
Primary/backup replication (also known as primary copy replication master-slave replication or log shipping) is perhaps the most commonly used replication method, and the most basic algorithm. All updated are performed on the primary, and a log of operations (or alternatively, changes) is shipped across the network to the backup replicas. There are two variants:
- asynchronous primary/backup replication and
- synchronous primary/backup replication
> 主/备份复制（也称为主副本复制、主从复制或日志运输）可能是最常用的复制方法，也是最基本的算法。所有更新都在主副本上进行，操作日志（或者说更改日志）通过网络传送到备份副本。有两种变体：
	异步主/备份复制和
	同步主/备份复制

The synchronous version requires two messages ("update" + "acknowledge receipt") while the asynchronous version could run with just one ("update").
> 同步版本需要两条信息（"更新 "+"确认收到"），而异步版本只需一条信息（"更新"）即可运行。

P/B is very common. For example, by default MySQL replication uses the asynchronous variant. MongoDB also uses P/B (with some additional procedures for failover). All operations are performed on one master server, which serializes them to a local log, which is then replicated asynchronously to the backup servers.
> P/B 非常常见。例如，MySQL 复制默认使用异步变体。MongoDB 也使用 P/B（还有一些用于故障转移的附加程序）。所有操作都在一台主服务器上执行，主服务器将操作序列化到本地日志，然后异步复制到备份服务器。

As we discussed earlier in the context of asynchronous replication, any asynchronous replication algorithm can only provide weak durability guarantees. In MySQL replication this manifests as replication lag: the asynchronous backups are always at least one operation behind the primary. If the primary fails, then the updates that have not yet been sent to the backups are lost.
> 正如我们之前在异步复制中讨论的那样，任何异步复制算法都只能提供微弱的耐用性保证。在 MySQL 复制中，这表现为复制滞后：异步备份总是至少比主备份滞后一个操作。如果主数据库发生故障，那么尚未发送到备份的更新就会丢失。

The synchronous variant of primary/backup replication ensures that writes have been stored on other nodes before returning back to the client - at the cost of waiting for responses from other replicas. However, it is worth noting that even this variant can only offer weak guarantees. Consider the following simple failure scenario:
- the primary receives a write and sends it to the backup
- the backup persists and ACKs the write
- and then primary fails before sending ACK to the client
> 主/备份复制的同步变体可确保写入内容在返回客户端之前已存储在其他节点上，但代价是需要等待其他复制的响应。但值得注意的是，即使是这种变体，也只能提供微弱的保证。请考虑以下简单的故障情况：
	主副本接收到写操作并将其发送给备份
	备份持久化并 ACK 该写操作
	然后主服务器在向客户端发送 ACK 之前发生故障

The client now assumes that the commit failed, but the backup committed it; if the backup is promoted to primary, it will be incorrect. Manual cleanup may be needed to reconcile the failed primary or divergent backups.
> 客户端现在假定提交失败，但备份已提交；如果将备份提升为主备份，则提交将是错误的。可能需要手动清理，以协调失败的主备份或不同的备份。

I am simplifying here of course. While all primary/backup replication algorithms follow the same general messaging pattern, they differ in their handling of failover, replicas being offline for extended periods and so on. However, it is not possible to be resilient to inopportune failures of the primary in this scheme.
> 当然，我在这里做了简化。虽然所有主/备份复制算法都遵循相同的一般消息传递模式，但它们在处理故障转移、副本长时间离线等问题上有所不同。不过，在这种方案中，主服务器不可能对不恰当的故障具有弹性。

What is key in the log-shipping / primary/backup based schemes is that they can only offer a best-effort guarantee (e.g. they are susceptible to lost updates or incorrect updates if nodes fail at inopportune times). Furthermore, P/B schemes are susceptible to split-brain, where the failover to a backup kicks in due to a temporary network issue and causes both the primary and backup to be active at the same time.
> 基于日志传输/主用/备份方案的关键在于，它们只能提供尽力而为的保证（例如，如果节点在不恰当的时间发生故障，它们很容易丢失更新或更新不正确）。此外，P/B 方案还容易出现 "分脑"（split-brain）现象，即由于临时网络问题导致故障切换到备份，从而使主用和备份同时处于活动状态。

To prevent inopportune failures from causing consistency guarantees to be violated; we need to add another round of messaging, which gets us the two phase commit protocol (2PC).
> 为了防止不合时宜的故障导致一致性保证被违反，我们需要增加一轮消息传递，这就得到了两阶段提交协议（2PC）。

## Two phase commit (2PC)
[Two phase commit](http://en.wikipedia.org/wiki/Two-phase_commit_protocol) (2PC) is a protocol used in many classic relational databases. For example, MySQL Cluster (not to be confused with the regular MySQL) provides synchronous replication using 2PC. The diagram below illustrates the message flow:
> 两阶段提交（2PC）是许多经典关系数据库中使用的一种协议。例如，MySQL Cluster（不要与普通 MySQL 混淆）使用 2PC 提供同步复制。下图说明了信息流：

```
[ Coordinator ] -> OK to commit?     [ Peers ]
                <- Yes / No

[ Coordinator ] -> Commit / Rollback [ Peers ]
                <- ACK
```

In the first phase (voting), the coordinator sends the update to all the participants. Each participant processes the update and votes whether to commit or abort. When voting to commit, the participants store the update onto a temporary area (the write-ahead log). Until the second phase completes, the update is considered temporary.
> 在第一阶段（投票），协调者向所有参与者发送更新。每个参与者处理更新并投票决定是提交还是放弃。当投票决定提交时，参与者会将更新存储到一个临时区域（先写日志）。在第二阶段完成之前，更新被视为临时的。

In the second phase (decision), the coordinator decides the outcome and informs every participant about it. If all participants voted to commit, then the update is taken from the temporary area and made permanent.
> 在第二阶段（决定），协调人决定结果并通知每个参与者。如果所有参与者都投票决定提交，那么更新就会从临时区域中移出，成为永久更新。

Having a second phase in place before the commit is considered permanent is useful, because it allows the system to roll back an update when a node fails. In contrast, in primary/backup ("1PC"), there is no step for rolling back an operation that has failed on some nodes and succeeded on others, and hence the replicas could diverge.
> 在提交被认为是永久性的之前有一个第二阶段是非常有用的，因为它允许系统在一个节点出现故障时回滚更新。相比之下，在主节点/备份节点（"1PC"）中，没有回滚在某些节点上失败而在其他节点上成功的操作的步骤，因此复制可能会出现偏差。

2PC is prone to blocking, since a single node failure (participant or coordinator) blocks progress until the node has recovered. Recovery is often possible thanks to the second phase, during which other nodes are informed about the system state. Note that 2PC assumes that the data in stable storage at each node is never lost and that no node crashes forever. Data loss is still possible if the data in the stable storage is corrupted in a crash.
> 2PC 容易出现阻塞，因为单个节点故障（参与者或协调者）会阻塞进程，直到该节点恢复。由于第二阶段会向其他节点通报系统状态，因此恢复通常是可能的。请注意，2PC 假设每个节点稳定存储的数据永远不会丢失，也没有节点永远崩溃。如果稳定存储器中的数据在崩溃时损坏，数据仍有可能丢失。

The details of the recovery procedures during node failures are quite complicated so I won't get into the specifics. The major tasks are ensuring that writes to disk are durable (e.g. flushed to disk rather than cached) and making sure that the right recovery decisions are made (e.g. learning the outcome of the round and then redoing or undoing an update locally).
> 节点故障期间恢复过程的细节相当复杂，因此我不会详细介绍。主要任务是确保对磁盘的写入是持久的（例如，刷新到磁盘而不是缓存），并确保做出正确的恢复决策（例如，了解轮次的结果，然后在本地重做或撤消更新）。

As we learned in the chapter regarding CAP, 2PC is a CA - it is not partition tolerant. The failure model that 2PC addresses does not include network partitions; the prescribed way to recover from a node failure is to wait until the network partition heals. There is no safe way to promote a new coordinator if one fails; rather a manual intervention is required. 2PC is also fairly latency-sensitive, since it is a write N-of-N approach in which writes cannot proceed until the slowest node acknowledges them.
> 正如我们在有关 CAP 的章节中所了解到的，2PC 是一种 CA - 它不能容忍分区。2PC 解决的故障模型不包括网络分区；从节点故障中恢复的规定方法是等待网络分区愈合。如果一个协调器失效，也没有安全的方法来提升一个新的协调器；而是需要人工干预。2PC 对延迟也相当敏感，因为它是一种 N-N 写入方法，在这种方法中，只有最慢的节点确认后才能继续写入。

2PC strikes a decent balance between performance and fault tolerance, which is why it has been popular in relational databases. However, newer systems often use a partition tolerant consensus algorithm, since such an algorithm can provide automatic recovery from temporary network partitions as well as more graceful handling of increased between-node latency.
> 2PC 在性能和容错之间取得了很好的平衡，因此在关系数据库中很受欢迎。不过，较新的系统通常使用分区容错共识算法，因为这种算法可以从临时网络分区中自动恢复，并能更从容地处理节点间延迟的增加。

## Partition tolerant consensus algorithms
Partition tolerant consensus algorithms are as far as we're going to go in terms of fault-tolerant algorithms that maintain single-copy consistency. There is a further class of fault tolerant algorithms: algorithms that tolerate arbitrary (Byzantine) faults; these include nodes that fail by acting maliciously. Such algorithms are rarely used in commercial systems, because they are more expensive to run and more complicated to implement - and hence I will leave them out.
> 在保持单拷贝一致性的容错算法方面，分区容错共识算法是我们所要研究的极限。还有一类容错算法：可容忍任意（拜占庭）故障的算法；其中包括恶意行为导致故障的节点。这类算法很少用于商业系统，因为它们的运行成本更高，实现起来也更复杂--因此我将不考虑它们。

When it comes to partition tolerant consensus algorithms, the most well-known algorithm is the Paxos algorithm. It is, however, notoriously difficult to implement and explain, so I will focus on Raft, a recent (~early 2013) algorithm designed to be easier to teach and implement. Let's first take a look at network partitions and the general characteristics of partition tolerant consensus algorithms.
> 说到分区容忍共识算法，最著名的是 Paxos 算法。不过，它在实现和解释上的难度是出了名的，因此我将重点介绍 Raft，这是一种最近（约 2013 年初）设计的算法，更易于教学和实现。让我们先来看看网络分区和分区容忍共识算法的一般特点。

### What is a network partition?
A network partition is the failure of a network link to one or several nodes. The nodes themselves continue to stay active, and they may even be able to receive requests from clients on their side of the network partition. As we learned earlier - during the discussion of the CAP theorem - network partitions do occur and not all systems handle them gracefully.
> 网络分区是指一个或多个节点的网络链接出现故障。节点本身会继续保持活动状态，甚至还能接收来自网络分区一侧客户端的请求。正如我们之前在讨论 CAP 定理时了解到的，网络分区确实会发生，但并非所有系统都能从容应对。

Network partitions are tricky because during a network partition, it is not possible to distinguish between a failed remote node and the node being unreachable. If a network partition occurs but no nodes fail, then the system is divided into two partitions which are simultaneously active. The two diagrams below illustrate how a network partition can look similar to a node failure.
> 网络分区非常棘手，因为在网络分区期间，无法区分远程节点故障和节点不可达。如果发生了网络分区，但没有节点发生故障，那么系统就会被分为两个同时处于活动状态的分区。下面两张图说明了网络分区与节点故障的相似之处。

A system of 2 nodes, with a failure vs. a network partition:
![[system-of-2 1.png]]

A system of 3 nodes, with a failure vs. a network partition:
![[system-of-3.png]]
A system that enforces single-copy consistency must have some method to break symmetry: otherwise, it will split into two separate systems, which can diverge from each other and can no longer maintain the illusion of a single copy.
> 强制执行单拷贝一致性的系统必须有某种方法来打破对称性：否则，它就会分裂成两个独立的系统，而这两个系统可能会相互背离，无法再保持单拷贝的假象。

Network partition tolerance for systems that enforce single-copy consistency requires that during a network partition, only one partition of the system remains active since during a network partition it is not possible to prevent divergence (e.g. CAP theorem).
> 执行单拷贝一致性的系统的网络分区容差要求在网络分区期间，系统只有一个分区保持活动状态，因为在网络分区期间不可能防止分歧（如 CAP 定理）。

### Majority decisions
This is why partition tolerant consensus algorithms rely on a majority vote. Requiring a majority of nodes - rather than all of the nodes (as in 2PC) - to agree on updates allows a minority of the nodes to be down, or slow, or unreachable due to a network partition. As long as (N/2 + 1)-of-N nodes are up and accessible, the system can continue to operate.
> 这就是为什么分区容忍共识算法依赖于多数投票的原因。要求大多数节点 - 而不是所有节点 （如 2PC） - 就更新达成一致，允许少数节点由于网络分区而关闭、运行缓慢或无法访问。只要 （N/2 + 1）-of-N 节点已启动且可访问，系统就可以继续运行。

Partition tolerant consensus algorithms use an odd number of nodes (e.g. 3, 5 or 7). With just two nodes, it is not possible to have a clear majority after a failure. For example, if the number of nodes is three, then the system is resilient to one node failure; with five nodes the system is resilient to two node failures.
> 分区容忍共识算法使用奇数节点（如 3、5 或 7）。如果只有两个节点，就不可能在故障后出现明显的多数。例如，如果节点数为 3，那么系统就能抵御一个节点的故障；如果节点数为 5，那么系统就能抵御两个节点的故障。

When a network partition occurs, the partitions behave asymmetrically. One partition will contain the majority of the nodes. Minority partitions will stop processing operations to prevent divergence during a network partition, but the majority partition can remain active. This ensures that only a single copy of the system state remains active.
> 发生网络分区时，分区的行为是不对称的。一个分区将包含大部分节点。少数分区将停止处理操作，以防止在网络分区期间出现分歧，但多数分区仍可保持激活状态。这样可以确保只有一个系统状态副本保持活动状态。

Majorities are also useful because they can tolerate disagreement: if there is a perturbation or failure, the nodes may vote differently. However, since there can be only one majority decision, a temporary disagreement can at most block the protocol from proceeding (giving up liveness) but it cannot violate the single-copy consistency criterion (safety property).
> 多数票之所以有用，还因为它们可以容忍分歧：如果出现扰动或故障，节点的投票结果可能不同。不过，由于只能有一个多数决定，暂时的分歧最多只能阻止协议继续进行（放弃有效性），但不能违反单副本一致性准则（安全属性）。

### Roles
There are two ways one might structure a system: all nodes may have the same responsibilities, or nodes may have separate, distinct roles.
> 有两种方法可以构建系统：所有节点可能具有相同的职责，或者节点可能具有单独的不同角色。

Consensus algorithms for replication generally opt for having distinct roles for each node. Having a single fixed leader or master server is an optimization that makes the system more efficient, since we know that all updates must pass through that server. Nodes that are not the leader just need to forward their requests to the leader.
> 用于复制的共识算法通常选择为每个节点具有不同的角色。拥有单个固定的 leader 或 master 服务器是一种优化，可以提高系统的效率，因为我们知道所有更新都必须通过该服务器。不是 leader 的节点只需将其请求转发给 leader 即可。

Note that having distinct roles does not preclude the system from recovering from the failure of the leader (or any other role). Just because roles are fixed during normal operation doesn't mean that one cannot recover from failure by reassigning the roles after a failure (e.g. via a leader election phase). Nodes can reuse the result of a leader election until node failures and/or network partitions occur.
> 请注意，具有不同的角色并不妨碍系统从 leader （或任何其他角色） 的故障中恢复。仅仅因为角色在正常操作期间是固定的，并不意味着不能通过在失败后重新分配角色（例如，通过领导者选举阶段）来从失败中恢复。节点可以重复使用领导者选举的结果，直到节点发生故障和/或发生网络分区。

Both Paxos and Raft make use of distinct node roles. In particular, they have a leader node ("proposer" in Paxos) that is responsible for coordination during normal operation. During normal operation, the rest of the nodes are followers ("acceptors" or "voters" in Paxos).
> Paxos 和 Raft 都使用不同的节点角色。特别是，它们有一个领导节点（在 Paxos 中为 “proposer”），负责在正常操作期间进行协调。在正常操作期间，其余节点是 follower（在 Paxos 中为 “acceptors” 或 “voters”）。

### Epochs
Each period of normal operation in both Paxos and Raft is called an epoch ("term" in Raft). During each epoch only one node is the designated leader (a similar system is used in Japan where era names change upon imperial succession).
> 在 Paxos 和 Raft 中，正常运行的每个时期都称为一个纪元（Raft 中称为 "term"）。在每个纪元中，只有一个节点被指定为领导者（日本也使用类似的系统，在皇室继承时，纪元名称会发生变化）。
![[epoch.png]]

After a successful election, the same leader coordinates until the end of the epoch. As shown in the diagram above (from the Raft paper), some elections may fail, causing the epoch to end immediately.
> 选举成功后，同一领导者会一直坐标到纪元结束。如上图所示（摘自 Raft 论文），有些选举可能会失败，导致纪元立即结束。

Epochs act as a logical clock, allowing other nodes to identify when an outdated node starts communicating - nodes that were partitioned or out of operation will have a smaller epoch number than the current one, and their commands are ignored.
> 纪元作为逻辑时钟，允许其他节点识别过时节点何时开始通信--被分区或停止运行的节点的纪元号将小于当前的纪元号，其命令将被忽略。

Leader changes via duels
During normal operation, a partition-tolerant consensus algorithm is rather simple. As we've seen earlier, if we didn't care about fault tolerance, we could just use 2PC. Most of the complexity really arises from ensuring that once a consensus decision has been made, it will not be lost and the protocol can handle leader changes as a result of a network or node failure.
> 在正常运行期间，分区容错共识算法相当简单。正如我们前面所看到的，如果我们不在乎容错，我们可以直接使用 2PC。实际上，大部分复杂性都来自于确保共识决策一旦做出，就不会丢失，并且协议能够处理因网络或节点故障而导致的领导者变更。

All nodes start as followers; one node is elected to be a leader at the start. During normal operation, the leader maintains a heartbeat which allows the followers to detect if the leader fails or becomes partitioned.
> 所有节点都从 follower 开始;一个节点在开始时被选为 Leader。在正常操作期间，leader 会维护一个心跳，这允许 follower 检测 leader 是否失败或被分区。

When a node detects that a leader has become non-responsive (or, in the initial case, that no leader exists), it switches to an intermediate state (called "candidate" in Raft) where it increments the term/epoch value by one, initiates a leader election and competes to become the new leader.
> 当节点检测到 leader 变得无响应（或者，在最初的情况下，不存在 leader）时，它会切换到中间状态（在 Raft 中称为 “candidate”），在该状态中，它将 term/epoch 值增加 1，启动 leader 选举并竞争成为新的 leader。

In order to be elected a leader, a node must receive a majority of the votes. One way to assign votes is to simply assign them on a first-come-first-served basis; this way, a leader will eventually be elected. Adding a random amount of waiting time between attempts at getting elected will reduce the number of nodes that are simultaneously attempting to get elected.
> 为了当选领导者，节点必须获得多数票。分配投票的一种方法是简单地按照先到先得的原则分配它们;这样，最终将选出一名领导者。在尝试当选之间添加随机的等待时间将减少同时尝试当选的节点数。

### Numbered proposals within an epoch
During each epoch, the leader proposes one value at a time to be voted upon. Within each epoch, each proposal is numbered with a unique strictly increasing number. The followers (voters / acceptors) accept the first proposal they receive for a particular proposal number.
> 在每个纪元中，领导者每次提出一个值供投票表决。在每个纪元中，每个提案都有一个唯一的严格递增编号。追随者（投票者/接受者）接受他们收到的第一个特定提案编号的提案。

### Normal operation
During normal operation, all proposals go through the leader node. When a client submits a proposal (e.g. an update operation), the leader contacts all nodes in the quorum. If no competing proposals exist (based on the responses from the followers), the leader proposes the value. If a majority of the followers accept the value, then the value is considered to be accepted.
> 在正常运行期间，所有提议都要经过领导者节点。当客户端提交建议（如更新操作）时，领导者会联系法定人数中的所有节点。如果没有竞争提案（基于跟随者的响应），领导者就会提出值。如果大多数跟随者都接受该值，则认为该值已被接受。

Since it is possible that another node is also attempting to act as a leader, we need to ensure that once a single proposal has been accepted, its value can never change. Otherwise a proposal that has already been accepted might for example be reverted by a competing leader. Lamport states this as:
	P2: If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v.
> 由于有可能另一个节点也在试图充当领导者，因此我们需要确保一旦某个提议被接受，其值就永远不会改变。否则，已经被接受的提议可能会被竞争领导者撤销。兰波特（Lamport）指出了这一点：
	P2：如果价值为 v 的提案被选中，那么被选中的每一个更高编号的提案的价值都是 v。

Ensuring that this property holds requires that both followers and proposers are constrained by the algorithm from ever changing a value that has been accepted by a majority. Note that "the value can never change" refers to the value of a single execution (or run / instance / decision) of the protocol. A typical replication algorithm will run multiple executions of the algorithm, but most discussions of the algorithm focus on a single run to keep things simple. We want to prevent the decision history from being altered or overwritten.
> 要确保这一属性成立，就要求追随者和提议者都受到算法的限制，不得改变已被多数人接受的值。请注意，"值永远不会改变 "指的是协议的单次执行（或运行/实例/决策）值。典型的复制算法会运行多次算法执行，但大多数算法讨论都集中在单次运行上，以保持简单。我们要防止决策历史被更改或覆盖。

In order to enforce this property, the proposers must first ask the followers for their (highest numbered) accepted proposal and value. If the proposer finds out that a proposal already exists, then it must simply complete this execution of the protocol, rather than making its own proposal. Lamport states this as:
	P2b. If a proposal with value `v` is chosen, then every higher-numbered proposal issued by any proposer has value `v`.
> 为了执行这一属性，提议者必须首先向追随者询问他们（编号最高的）已接受的提议和价值。如果提议者发现已有提议，那么它就必须完成协议的执行，而不是提出自己的提议。兰波特是这样说的：
	P2b.如果选择了具有 v 值的提议，那么任何提议者发出的每一个更高编号的提议都具有 v 值。

This is the core of the Paxos algorithm, as well as algorithms derived from it. The value to be proposed is not chosen until the second phase of the protocol. Proposers must sometimes simply retransmit a previously made decision to ensure safety (e.g. clause b in P2c) until they reach a point where they know that they are free to impose their own proposal value (e.g. clause a).
> This is the core of the Paxos algorithm, as well as algorithms derived from it. The value to be proposed is not chosen until the second phase of the protocol. Proposers must sometimes simply retransmit a previously made decision to ensure safety (e.g. clause b in P2c) until they reach a point where they know that they are free to impose their own proposal value (e.g. clause a).

If multiple previous proposals exist, then the highest-numbered proposal value is proposed. Proposers may only attempt to impose their own value if there are no competing proposals at all.
> 如果存在多个先前的提案，则建议编号最高的提案值。提议者只有在根本没有竞争性提案的情况下才能尝试强加自己的价值。

To ensure that no competing proposals emerge between the time the proposer asks each acceptor about its most recent value, the proposer asks the followers not to accept proposals with lower proposal numbers than the current one.
> 为了确保在提议者询问每个接受者其最新值之间没有竞争性提案出现，提议者要求追随者不接受提案编号低于当前提案编号的提案。

Putting the pieces together, reaching a decision using Paxos requires two rounds of communication:
```
[ Proposer ] -> Prepare(n)                                [ Followers ]
             <- Promise(n; previous proposal number
                and previous value if accepted a
                proposal in the past)

[ Proposer ] -> AcceptRequest(n, own value or the value   [ Followers ]
                associated with the highest proposal number
                reported by the followers)
                <- Accepted(n, value)
```

The prepare stage allows the proposer to learn of any competing or previous proposals. The second phase is where either a new value or a previously accepted value is proposed. In some cases - such as if two proposers are active at the same time (dueling); if messages are lost; or if a majority of the nodes have failed - then no proposal is accepted by a majority. But this is acceptable, since the decision rule for what value to propose converges towards a single value (the one with the highest proposal number in the previous attempt).
> 准备阶段允许提案者了解任何竞争或以前的提案。 第二阶段是提出新值或以前接受的值的地方。 在某些情况下-例如，如果两个提议者在同一时间处于活动状态（决斗）;如果消息丢失;或者如果大多数节点失败-那么大多数提议都不被接受。 但这是可以接受的，因为建议什么值的决策规则收敛于单个值（在前一次尝试中具有最高建议编号的值）。

Indeed, according to the FLP impossibility result, this is the best we can do: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold. Paxos gives up liveness: it may have to delay decisions indefinitely until a point in time where there are no competing leaders, and a majority of nodes accept a proposal. This is preferable to violating the safety guarantees.
> 事实上，根据 FLP 不可能性结果，这已经是我们能做的最好的结果了：当信息传递边界的保证不成立时，解决共识问题的算法必须要么放弃安全性，要么放弃有效性。Paxos 放弃了有效性：它可能不得不无限期地推迟决策，直到某个时间点上没有竞争的领导者，并且大多数节点都接受了某个提议。这比违反安全保证更可取。

Of course, implementing this algorithm is much harder than it sounds. There are many small concerns which add up to a fairly significant amount of code even in the hands of experts. These are issues such as:
- practical optimizations:
    - avoiding repeated leader election via leadership leases (rather than heartbeats)
    - avoiding repeated propose messages when in a stable state where the leader identity does not change
- ensuring that followers and proposers do not lose items in stable storage and that results stored in stable storage are not subtly corrupted (e.g. disk corruption)
- enabling cluster membership to change in a safe manner (e.g. base Paxos depends on the fact that majorities always intersect in one node, which does not hold if the membership can change arbitrarily)
- procedures for bringing a new replica up to date in a safe and efficient manner after a crash, disk loss or when a new node is provisioned
- procedures for snapshotting and garbage collecting the data required to guarantee safety after some reasonable period (e.g. balancing storage requirements and fault tolerance requirements)
> 当然，实现这种算法比听起来要难得多。即使是在专家手中，也会有许多小问题导致代码量相当大。这些问题包括
	实际优化
		通过领导租约（而不是心跳）避免重复选举领导者
		在领导者身份不变的稳定状态下，避免重复发送提议信息
	确保追随者和提议者不会丢失稳定存储中的项目，并确保存储在稳定存储中的结果不会受到微妙的损坏（如磁盘损坏）
	使集群成员以安全的方式发生变化（例如，基本 Paxos 依赖于多数总是在一个节点相交这一事实，而如果成员可任意变化，这一事实就不成立了）
	在发生崩溃、磁盘丢失或配置新节点后，以安全高效的方式更新新副本的程序
	在一段合理的时间后，对保证安全性所需的数据进行快照和垃圾收集的程序（例如，平衡存储要求和容错要求）

## Partition-tolerant consensus algorithms: Paxos, Raft, ZAB
Hopefully, this has given you a sense of how a partition-tolerant consensus algorithm works. I encourage you to read one of the papers in the further reading section to get a grasp of the specifics of the different algorithms.
> 希望这能让你对分区容忍共识算法的工作原理有所了解。我鼓励你阅读 "进一步阅读 "部分的论文，以了解不同算法的具体细节。

_Paxos_. Paxos is one of the most important algorithms when writing strongly consistent partition tolerant replicated systems. It is used in many of Google's systems, including the [Chubby lock manager](http://research.google.com/archive/chubby.html) used by [BigTable](http://research.google.com/archive/bigtable.html)/[Megastore](http://research.google.com/pubs/pub36971.html), the Google File System as well as [Spanner](http://research.google.com/archive/spanner.html).
> Paxos：Paxos 是编写强一致性分区容错复制系统时最重要的算法之一。谷歌的许多系统都使用了该算法，包括 BigTable/Megastore 使用的 Chubby 锁管理器、谷歌文件系统以及 Spanner。

Paxos is named after the Greek island of Paxos, and was originally presented by Leslie Lamport in a paper called "The Part-Time Parliament" in 1998. It is often considered to be difficult to implement, and there have been a series of papers from companies with considerable distributed systems expertise explaining further practical details (see the further reading). You might want to read Lamport's commentary on this issue here and here.
> Paxos 以希腊的 Paxos 岛命名，最初由 Leslie Lamport 于 1998 年在一篇名为“兼职议会”的论文中提出。它通常被认为难以实现，并且具有大量分布式系统专业知识的公司已经发表了一系列论文，解释了更多的实际细节（请参阅进一步阅读）。您可能想阅读 Lamport 对此问题的评论 这里 和 这里.

The issues mostly relate to the fact that Paxos is described in terms of a single round of consensus decision making, but an actual working implementation usually wants to run multiple rounds of consensus efficiently. This has led to the development of many extensions on the core protocol that anyone interested in building a Paxos-based system still needs to digest. Furthermore, there are additional practical challenges such as how to facilitate cluster membership change.
> 这些问题主要与这样一个事实有关，即 Paxos 是以单轮共识决策的方式来描述的，但实际的工作实施通常希望高效地运行多轮共识。这就导致在核心协议的基础上开发了许多扩展协议，而任何有兴趣构建基于 Paxos 系统的人都还需要消化这些扩展协议。此外，还有一些额外的实际挑战，例如如何促进群组成员的变更。

ZAB. ZAB - the Zookeeper Atomic Broadcast protocol is used in Apache Zookeeper. Zookeeper is a system which provides coordination primitives for distributed systems, and is used by many Hadoop-centric distributed systems for coordination (e.g. HBase, Storm, Kafka). Zookeeper is basically the open source community's version of Chubby. Technically speaking atomic broadcast is a problem different from pure consensus, but it still falls under the category of partition tolerant algorithms that ensure strong consistency.
> ZAB：ZAB - Zookeeper 原子广播协议用于 Apache Zookeeper。Zookeeper 是一个为分布式系统提供协调原语的系统，许多以 Hadoop 为中心的分布式系统（如 HBase、Storm、Kafka）都使用它来进行协调。Zookeeper 基本上是开源社区版本的 Chubby。从技术上讲，原子广播是一个不同于纯共识的问题，但它仍属于确保强一致性的分区容忍算法。

Raft. Raft is a recent (2013) addition to this family of algorithms. It is designed to be easier to teach than Paxos, while providing the same guarantees. In particular, the different parts of the algorithm are more clearly separated and the paper also describes a mechanism for cluster membership change. It has recently seen adoption in etcd inspired by ZooKeeper.
> Raft：Raft 是最近（2013 年）加入该算法系列的新算法。与 Paxos 相比，它的设计更易于教学，同时提供相同的保证。特别是，算法的不同部分被更清晰地分开，论文还描述了一种群成员变更机制。最近，etcd 在 ZooKeeper 的启发下采用了该算法。

## Replication methods with strong consistency
In this chapter, we took a look at replication methods that enforce strong consistency. Starting with a contrast between synchronous work and asynchronous work, we worked our way up to algorithms that are tolerant of increasingly complex failures. Here are some of the key characteristics of each of the algorithms:
> 在本章中，我们将介绍可执行强一致性的复制方法。从同步工作和异步工作的对比开始，我们逐步深入到可容忍日益复杂故障的算法。以下是每种算法的一些主要特点：

#### Primary/Backup
- Single, static master
- Replicated log, slaves are not involved in executing operations
- No bounds on replication delay
- Not partition tolerant
- Manual/ad-hoc failover, not fault tolerant, "hot backup"
> 
	单一、静态主日志
	复制日志，从机不参与执行操作
	复制延迟无限制
	不支持分区
	手动/临时故障切换，不容错，"热备份

#### 2PC
- Unanimous vote: commit or abort
- Static master
- 2PC cannot survive simultaneous failure of the coordinator and a node during a commit
- Not partition tolerant, tail latency sensitive
> 
	一致投票：提交或中止
	静态主控
	2PC 无法在提交期间 coordinator 和节点同时发生故障
	不容忍分区，对尾部延迟敏感

#### Paxos
- Majority vote
- Dynamic master
- Robust to n/2-1 simultaneous failures as part of protocol
- Less sensitive to tail latency
> 
	多数票
	动态主控
	作为协议的一部分，对 n/2-1 同时故障具有鲁棒性
	对尾部延迟不太敏感

# 5. Replication: weak consistency model protocols
Now that we've taken a look at protocols that can enforce single-copy consistency under an increasingly realistic set of supported failure cases, let's turn our attention at the world of options that opens up once we let go of the requirement of single-copy consistency.
> 既然我们已经了解了能够在越来越现实的故障情况下强制执行单拷贝一致性的协议，那么让我们把注意力转移到一旦我们放弃单拷贝一致性的要求后所出现的各种选择上吧。

By and large, it is hard to come up with a single dimension that defines or characterizes the protocols that allow for replicas to diverge. Most such protocols are highly available, and the key issue is more whether or not the end users find the guarantees, abstractions and APIs useful for their purpose in spite of the fact that the replicas may diverge when node and/or network failures occur.
> 总的来说，很难找到一个单一的维度来定义或描述允许副本分化的协议。大多数此类协议都具有很高的可用性，而关键问题更多在于，尽管在节点和/或网络发生故障时，副本可能会出现分歧，但最终用户是否认为这些保证、抽象和应用程序接口对他们的目的有用。

Why haven't weakly consistent systems been more popular?
> 为什么弱一致性系统没有更受欢迎？

As I stated in the introduction, I think that much of distributed programming is about dealing with the implications of two consequences of distribution:
- that information travels at the speed of light
- that independent things fail independently
> 正如我在引言中所说，我认为分布式编程的大部分内容都是关于处理分布式的两个后果的影响：
	信息以光速传播
	独立事物独立失败

The implication that follows from the limitation on the speed at which information travels is that nodes experience the world in different, unique ways. Computation on a single node is easy, because everything happens in a predictable global total order. Computation on a distributed system is difficult, because there is no global total order.
> 对信息传播速度的限制意味着节点以不同的、独特的方式体验世界。在单个节点上进行计算很容易，因为一切都以可预测的全局总顺序进行。在分布式系统上进行计算很困难，因为没有全局总序。

For the longest while (e.g. decades of research), we've solved this problem by introducing a global total order. I've discussed the many methods for achieving strong consistency by creating order (in a fault-tolerant manner) where there is no naturally occurring total order.
> 长期以来（比如几十年的研究），我们通过引入全局总序来解决这个问题。我已经讨论过很多方法，通过在没有天然总序的地方创建有序（以容错的方式）来实现强一致性。

Of course, the problem is that enforcing order is expensive. This breaks down in particular with large scale internet systems, where a system needs to remain available. A system enforcing strong consistency doesn't behave like a distributed system: it behaves like a single system, which is bad for availability during a partition.
> 当然，问题在于执行命令的成本很高。这在需要保持系统可用性的大型互联网系统中尤为明显。执行强一致性的系统不会像分布式系统那样运行：它会像单个系统那样运行，这对分区期间的可用性很不利。

Furthermore, for each operation, often a majority of the nodes must be contacted - and often not just once, but twice (as you saw in the discussion on 2PC). This is particularly painful in systems that need to be geographically distributed to provide adequate performance for a global user base.
> 此外，对于每个操作，通常必须联系大多数节点 - 而且通常不仅仅是一次，而是两次（正如您在 2PC 上的讨论中看到的那样）。在需要地理分布以便为全球用户群提供足够性能的系统中，这尤其令人痛苦。

So behaving like a single system by default is perhaps not desirable.
> 因此，默认情况下的单一系统行为也许并不可取。

Perhaps what we want is a system where we can write code that doesn't use expensive coordination, and yet returns a "usable" value. Instead of having a single truth, we will allow different replicas to diverge from each other - both to keep things efficient but also to tolerate partitions - and then try to find a way to deal with the divergence in some manner.
> 也许我们想要的是一个系统，我们可以编写不使用昂贵的协调，但返回"可用"值的代码。 我们不会有一个单一的事实，而是允许不同的副本彼此分歧-既要保持事情的效率，又要容忍分区-然后试图找到一种方法来以某种方式处理分歧。

Eventual consistency expresses this idea: that nodes can for some time diverge from each other, but that eventually they will agree on the value.
> 最终一致性表达了这一理念：节点之间在一段时间内可能会出现分歧，但最终它们会在值上达成一致。

Within the set of systems providing eventual consistency, there are two types of system designs:
> 在提供最终一致性的系统集合中，有两类系统设计：

Eventual consistency with probabilistic guarantees. This type of system can detect conflicting writes at some later point, but does not guarantee that the results are equivalent to some correct sequential execution. In other words, conflicting updates will sometimes result in overwriting a newer value with an older one and some anomalies can be expected to occur during normal operation (or during partitions).
> 具有概率保证的最终一致性。这类系统能在稍后检测到冲突写入，但不能保证结果等同于某种正确的顺序执行。换句话说，冲突更新有时会导致用较旧的值覆盖较新的值，在正常运行过程中（或分区过程中）可能会出现一些异常情况。

In recent years, the most influential system design offering single-copy consistency is Amazon's Dynamo, which I will discuss as an example of a system that offers eventual consistency with probabilistic guarantees.
> 近年来，提供单副本一致性的最有影响力的系统设计是亚马逊的 Dynamo，我将以它为例，讨论提供概率保证的最终一致性的系统。

Eventual consistency with strong guarantees. This type of system guarantees that the results converge to a common value equivalent to some correct sequential execution. In other words, such systems do not produce in anomalous results; without any coordination you can build replicas of the same service, and those replicas can communicate in any pattern and receive the updates in any order, and they will eventually agree on the end result as long as they all see the same information.
> 强保证的最终一致性。这类系统能保证结果收敛到一个共同值，相当于某个正确的顺序执行。换句话说，这类系统不会产生异常结果；无需任何协调，你就可以为同一服务建立多个副本，这些副本可以以任何模式进行通信，并以任何顺序接收更新，只要它们看到的信息相同，最终结果就会一致。

CRDT's (convergent replicated data types) are data types that guarantee convergence to the same value in spite of network delays, partitions and message reordering. They are provably convergent, but the data types that can be implemented as CRDT's are limited.
> CRDT（收敛复制数据类型）是一种数据类型，它能保证在网络延迟、分区和信息重排序的情况下收敛到相同的值。它们具有可证明的收敛性，但能作为 CRDT 实现的数据类型有限。

The CALM (consistency as logical monotonicity) conjecture is an alternative expression of the same principle: it equates logical monotonicity with convergence. If we can conclude that something is logically monotonic, then it is also safe to run without coordination. Confluence analysis - in particular, as applied for the Bloom programming language - can be used to guide programmer decisions about when and where to use the coordination techniques from strongly consistent systems and when it is safe to execute without coordination.
> CALM（作为逻辑单调性的一致性）猜想是同一原则的另一种表达方式：它将逻辑单调性等同于收敛性。如果我们可以得出结论，某件事情在逻辑上是单调的，那么它也可以在不协调的情况下安全运行。汇合分析--尤其是应用于 Bloom 编程语言的汇合分析--可用于指导程序员决定何时何地使用强一致性系统中的协调技术，以及何时无需协调即可安全执行。

Reconciling different operation orders
Perhaps the most obvious characteristic of systems that do not enforce single-copy consistency is that they allow replicas to diverge from each other. This means that there is no strictly defined pattern of communication: replicas can be separated from each other and yet continue to be available and accept writes.
> 不执行单拷贝一致性的系统最明显的特点可能就是允许复制彼此分离。这意味着没有严格定义的通信模式：副本可以彼此分离，但仍然可用并接受写入。

Let's imagine a system of three replicas, each of which is partitioned from the others. For example, the replicas might be in different datacenters and for some reason unable to communicate. Each replica remains available during the partition, accepting both reads and writes from some set of clients:
> 让我们设想一个由三个副本组成的系统，每个副本都与其他副本分区。例如，这些副本可能位于不同的数据中心，由于某种原因无法通信。在分区期间，每个副本都保持可用，接受来自某些客户端的读取和写入：
```
[Clients]   - > [A]

--- Partition ---

[Clients]   - > [B]

--- Partition ---

[Clients]   - > [C]
```

After some time, the partitions heal and the replica servers exchange information. They have received different updates from different clients and have diverged each other, so some sort of reconciliation needs to take place. What we would like to happen is that all of the replicas converge to the same result.
> 一段时间后，分区愈合，复制服务器交换信息。它们从不同的客户端接收到不同的更新，彼此产生了分歧，因此需要进行某种调节。我们希望所有副本的结果都趋于一致。
```
[A] \
    --> [merge]
[B] /     |
          |
[C] ----[merge]---> result
```

Another way to think about systems with weak consistency guarantees is to imagine a set of clients sending messages to two in some order. Because there is no coordination protocol that enforces a single total order, the messages can get delivered in different orders at the two replicas:
> 考虑具有弱一致性保证的系统的另一种方法是想象一组客户端以某种顺序向两个客户端发送消息。 由于没有强制执行单个总订单的协调协议，因此消息可以在两个副本处以不同的顺序传递:
```
[Clients]  --> [A]  1, 2, 3
[Clients]  --> [B]  2, 3, 1
```

This is, in essence, the reason why we need coordination protocols. For example, assume that we are trying to concatenate a string and the operations in messages 1, 2 and 3 are:
> 从本质上讲，这就是我们需要协调协议的原因。例如，假设我们要连接一个字符串，信息 1、2 和 3 中的操作分别是
```
1: { operation: concat('Hello ') }
2: { operation: concat('World') }
3: { operation: concat('!') }
```

Then, without coordination, A will produce "Hello World!", and B will produce "World!Hello ".
```
A: concat(concat(concat('', 'Hello '), 'World'), '!') = 'Hello World!'
B: concat(concat(concat('', 'World'), '!'), 'Hello ') = 'World!Hello '
```

This is, of course, incorrect. Again, what we'd like to happen is that the replicas converge to the same result.

## Amazon's Dynamo
Amazon's Dynamo system design (2007) is probably the best-known system that offers weak consistency guarantees but high availability. It is the basis for many other real world systems, including LinkedIn's Voldemort, Facebook's Cassandra and Basho's Riak.

Dynamo is an eventually consistent, highly available key-value store. A key value store is like a large hash table: a client can set values via `set(key, value)` and retrieve them by key using `get(key)`. A Dynamo cluster consists of N peer nodes; each node has a set of keys which is it responsible for storing.

Dynamo prioritizes availability over consistency; it does not guarantee single-copy consistency. Instead, replicas may diverge from each other when values are written; when a key is read, there is a read reconciliation phase that attempts to reconcile differences between replicas before returning the value back to the client.
> Dynamo优先考虑可用性而不是一致性；它不保证单副本一致性。 相反，当写入值时，副本可能会彼此不同；当读取键时，会有一个读取协调阶段，在将值返回给客户端之前尝试协调副本之间的差异。

For many features on Amazon, it is more important to avoid outages than it is to ensure that data is perfectly consistent, as an outage can lead to lost business and a loss of credibility. Furthermore, if the data is not particularly important, then a weakly consistent system can provide better performance and higher availability at a lower cost than a traditional RDBMS.

Since Dynamo is a complete system design, there are many different parts to look at beyond the core replication task. The diagram below illustrates some of the tasks; notably, how a write is routed to a node and written to multiple replicas.
```
[ Client ]
    |
( Mapping keys to nodes )
    |
    V
[ Node A ]
    |     \
( Synchronous replication task: minimum durability )
    |        \
[ Node B]  [ Node C ]
    A
    |
( Conflict detection; asynchronous replication task:
  ensuring that partitioned / recovered nodes recover )
    |
    V
[ Node D]
```

After looking at how a write is initially accepted, we'll look at how conflicts are detected, as well as the asynchronous replica synchronization task. This task is needed because of the high availability design, in which nodes may be temporarily unavailable (down or partitioned). The replica synchronization task ensures that nodes can catch up fairly rapidly even after a failure.

### Consistent hashing
Whether we are reading or writing, the first thing that needs to happen is that we need to locate where the data should live on the system. This requires some type of key-to-node mapping.
> 无论是读取还是写入，我们首先需要确定数据在系统中的位置。这就需要某种键到节点的映射。

In Dynamo, keys are mapped to nodes using a hashing technique known as consistent hashing (which I will not discuss in detail). The main idea is that a key can be mapped to a set of nodes responsible for it by a simple calculation on the client. This means that a client can locate keys without having to query the system for the location of each key; this saves system resources as hashing is generally faster than performing a remote procedure call.
> 在 Dynamo 中，键使用称为一致性哈希的哈希技术映射到节点（我不会详细讨论）。主要思想是，通过在客户端上进行简单的计算，可以将 key 映射到一组负责它的节点。这意味着客户端可以找到密钥，而无需向系统查询每个密钥的位置;这样可以节省系统资源，因为哈希通常比执行远程过程调用更快。

Partial quorums
Once we know where a key should be stored, we need to do some work to persist the value. This is a synchronous task; the reason why we will immediately write the value onto multiple nodes is to provide a higher level of durability (e.g. protection from the immediate failure of a node).

Just like Paxos or Raft, Dynamo uses quorums for replication. However, Dynamo's quorums are sloppy (partial) quorums rather than strict (majority) quorums.

Informally, a strict quorum system is a quorum system with the property that any two quorums (sets) in the quorum system overlap. Requiring a majority to vote for an update before accepting it guarantees that only a single history is admitted since each majority quorum must overlap in at least one node. This was the property that Paxos, for example, relied on.

Partial quorums do not have that property; what this means is that a majority is not required and that different subsets of the quorum may contain different versions of the same data. The user can choose the number of nodes to write to and read from:
- the user can choose some number W-of-N nodes required for a write to succeed; and
- the user can specify the number of nodes (R-of-N) to be contacted during a read.

`W` and `R` specify the number of nodes that need to be involved to a write or a read. Writing to more nodes makes writes slightly slower but increases the probability that the value is not lost; reading from more nodes increases the probability that the value read is up to date.

The usual recommendation is that `R + W > N`, because this means that the read and write quorums overlap in one node - making it less likely that a stale value is returned. A typical configuration is `N = 3` (e.g. a total of three replicas for each value); this means that the user can choose between:
 R = 1, W = 3;
 R = 2, W = 2 or
 R = 3, W = 1

More generally, again assuming `R + W > N`:
- `R = 1`, `W = N`: fast reads, slow writes
- `R = N`, `W = 1`: fast writes, slow reads
- `R = N/2` and `W = N/2 + 1`: favorable to both

N is rarely more than 3, because keeping that many copies of large amounts of data around gets expensive!

As I mentioned earlier, the Dynamo paper has inspired many other similar designs. They all use the same partial quorum based replication approach, but with different defaults for N, W and R:
- Basho's Riak (N = 3, R = 2, W = 2 default)
- Linkedin's Voldemort (N = 2 or 3, R = 1, W = 1 default)
- Apache's Cassandra (N = 3, R = 1, W = 1 default)

### Is R + W > N the same as "strong consistency"?
No.

### Conflict detection and read repair
Systems that allow replicas to diverge must have a way to eventually reconcile two different values. As briefly mentioned during the partial quorum approach, one way to do this is to detect conflicts at read time, and then apply some conflict resolution method. But how is this done?

In general, this is done by tracking the causal history of a piece of data by supplementing it with some metadata. Clients must keep the metadata information when they read data from the system, and must return back the metadata value when writing to the database.

We've already encountered a method for doing this: vector clocks can be used to represent the history of a value. Indeed, this is what the original Dynamo design uses for detecting conflicts.

However, using vector clocks is not the only alternative. If you look at many practical system designs, you can deduce quite a bit about how they work by looking at the metadata that they track.

_No metadata_. When a system does not track metadata, and only returns the value (e.g. via a client API), it cannot really do anything special about concurrent writes. A common rule is that the last writer wins: in other words, if two writers are writing at the same time, only the value from the slowest writer is kept around.

_Timestamps_. Nominally, the value with the higher timestamp value wins. However, if time is not carefully synchronized, many odd things can happen where old data from a system with a faulty or fast clock overwrites newer values. Facebook's Cassandra is a Dynamo variant that uses timestamps instead of vector clocks.

_Version numbers_. Version numbers may avoid some of the issues related with using timestamps. Note that the smallest mechanism that can accurately track causality when multiple histories are possible are vector clocks, not version numbers.

_Vector clocks_. Using vector clocks, concurrent and out of date updates can be detected. Performing read repair then becomes possible, though in some cases (concurrent changes) we need to ask the client to pick a value. This is because if the changes are concurrent and we know nothing more about the data (as is the case with a simple key-value store), then it is better to ask than to discard data arbitrarily.

When reading a value, the client contacts `R` of `N` nodes and asks them for the latest value for a key. It takes all the responses, discards the values that are strictly older (using the vector clock value to detect this). If there is only one unique vector clock + value pair, it returns that. If there are multiple vector clock + value pairs that have been edited concurrently (e.g. are not comparable), then all of those values are returned.

As is obvious from the above, read repair may return multiple values. This means that the client / application developer must occasionally handle these cases by picking a value based on some use-case specific criterion.

In addition, a key component of a practical vector clock system is that the clocks cannot be allowed to grow forever - so there needs to be a procedure for occasionally garbage collecting the clocks in a safe manner to balance fault tolerance with storage requirements.

### Replica synchronization: gossip and Merkle trees
Given that the Dynamo system design is tolerant of node failures and network partitions, it needs a way to deal with nodes rejoining the cluster after being partitioned, or when a failed node is replaced or partially recovered.

Replica synchronization is used to bring nodes up to date after a failure, and for periodically synchronizing replicas with each other.

Gossip is a probabilistic technique for synchronizing replicas. The pattern of communication (e.g. which node contacts which node) is not determined in advance. Instead, nodes have some probability p of attempting to synchronize with each other. Every t seconds, each node picks a node to communicate with. This provides an additional mechanism beyond the synchronous task (e.g. the partial quorum writes) which brings the replicas up to date.

Gossip is scalable, and has no single point of failure, but can only provide probabilistic guarantees.

In order to make the information exchange during replica synchronization efficient, Dynamo uses a technique called Merkle trees, which I will not cover in detail. The key idea is that a data store can be hashed at multiple different level of granularity: a hash representing the whole content, half the keys, a quarter of the keys and so on.

By maintaining this fairly granular hashing, nodes can compare their data store content much more efficiently than a naive technique. Once the nodes have identified which keys have different values, they exchange the necessary information to bring the replicas up to date.

### Dynamo in practice: probabilistically bounded staleness (PBS)
And that pretty much covers the Dynamo system design:

consistent hashing to determine key placement
partial quorums for reading and writing
conflict detection and read repair via vector clocks and
gossip for replica synchronization
How might we characterize the behavior of such a system? A fairly recent paper from Bailis et al. (2012) describes an approach called PBS (probabilistically bounded staleness) uses simulation and data collected from a real world system to characterize the expected behavior of such a system.

PBS estimates the degree of inconsistency by using information about the anti-entropy (gossip) rate, the network latency and local processing delay to estimate the expected level of consistency of reads. It has been implemented in Cassandra, where timing information is piggybacked on other messages and an estimate is calculated based on a sample of this information in a Monte Carlo simulation.

Based on the paper, during normal operation eventually consistent data stores are often faster and can read a consistent state within tens or hundreds of milliseconds. The table below illustrates amount of time required from a 99.9% probability of consistent reads given different R and W settings on empirical timing data from LinkedIn (SSD and 15k RPM disks) and Yammer:
![[pbs.png]]

For example, going from `R=1`, `W=1` to `R=2`, `W=1` in the Yammer case reduces the inconsistency window from 1352 ms to 202 ms - while keeping the read latencies lower (32.6 ms) than the fastest strict quorum (`R=3`, `W=1`; 219.27 ms).

For more details, have a look at the [PBS website](http://pbs.cs.berkeley.edu/) and the associated paper.

## Disorderly programming
